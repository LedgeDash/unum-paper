\section{Design}

To achieve the objectives in \S\ref{sec:bg}, \name{} utilizes a
design/strategy we call ``\emph{decentralized orchestration}'' where instead
of executing workflow orchestration logic entirely within a centralized
orchestrator, a set of ``decentralized orchestrators'' run \emph{in-situ} with
user functions and each performs only the orchestration logic \emph{local to
its subsection} of the workflow.

Efficiently implementing decentralized orchestration while also preserving the
benefits of centralized orchestrators (\S\ref{sec:bg:orchestrator}) requires
\name{} to solve three key challenges:

\begin{enumerate}

	\item Given a workflow written in a high-level description language, how
	to partition its orchestration logic such that it can run in a
	decentralized manner in-situ with user functions?\dhl{Sync with
	background, workflow definitions \emph{logically} expresses a directed
	graph}

	\item How to efficiently execute orchestration patterns that require data
	sharing and synchronization across function instances (e.g., fan-in) in a
	decentralized manner?\dhl{Sync with background. explain the challenges of
	fan-in in background.}

	\item How to provide exactly-once execution semantics when the
	orchestration logic is decentralized across function instances that can
	crash and retry at any point mid-execution?\dhl{Sync with background. Need
	to have talked about retries and duplicates already.}

\end{enumerate}

\name{} contributes an end-to-end system that solves the three challenges and
delivers better performance and cost savings than a state-of-the-art
production orchestrator (\S\ref{sec:eval}). Figure~\ref{fig:arch} depicts
\name{}'s architecture graphically.

\name{} solves the first challenge \emph{at compile time}, using a frontend
compiler and an intermediate representation (IR). Given a workflow definition
written in a high-level description language, the frontend compiler derives a
directed graph representation where nodes are constituent functions of the
workflow and edges represent workflow transitions between
functions. Based on the directed graph, the compiler generates an IR in
the form of configuration files, one file for each node in the graph. A
\name{} configuration encodes all the outgoing edges of a node such that each
function in the workflow knows how to transition just to its immediate
downstream nodes.

With the configurations from the IR, \name{} solves the second challenge using
a decentralized orchestrator (\deorc) that efficiently implements a set of
workflow patterns in a decentralized manner and can run in-situ with the
constituent user functions. In particular, the \deorc{} is a platform-specific
runtime library made up of an ingress component and a egress component.
Developers package each user function with its assigned \name{} configuration
and the \deorc{} to create an \emph{unumized} function. When an unumized
function executes, its entry point is no longer the user code but instead the
\deorc{} ingress. And when user code completes, it returns its results to the
\deorc{} egress which then interprets its co-located \name{} configuration and
performs the workflow transition.

A critical complexity is to enable fan-in patterns where a single ``sink''
function aggregates the results of multiple upstream functions. Importantly,
to avoid idle-billing (\S\ref{sec:bg}), we want the entire fan-in pattern to
execute \emph{wait-free}, i.e., to invoke the sink function only when all its
input upstream functions have completed. To this end, \name{} leverages a
shared \emph{intermediary data store} that is a strongly consistent serverless
storage (e.g., DynamoDB) with conditional store operations. Upstream functions
each writes user code output into and synchronizes over the intermediary data
store such that faster functions simply terminate and only the last-to-finish
function invokes the sink function.

\name{} leverages again the intermediary data store to also solve the third
challenge and provide \emph{exactly-once} execution semantics. \name{} uses
checkpointing, where each function checkpoints its output to the intermediary
data store, to limit the scope of retries when workflows crash mid-execution
and guarantee that even if there are multiple instances of the same function,
concurrent or not, only one instance's output is taken as the final result and
propargates downstream.


\input{architecture}
\input{patterns}
\input{ir-design}



\subsection{\name{} Runtime}

\begin{figure}[]
    \begin{minted}[
    frame=single,
    fontsize=\scriptsize
  ]{json}
{
    "Data": {
        "Source": "http | dynamodb | s3 | ...",
        "Value": "<object> | [<pointers>]"
    },
    "Session": "uuid",
    "Fan-out": {
        "Index": "int",
        "Size": "int",
        "OuterLoop": {
            "Index": "int",
            "Size": "int"
        }
    }
}
    \end{minted}
    \caption{\name{} runtime input payload schema}
    \label{fig:input-format}
\end{figure}

The primary purpose of the \name{} runtime is to implement the transition with
platform-specific APIs and manage runtime metadata that is required by the
\name{} IR naming scheme and programmable constructs. We design the runtime to
wrap user code and transparently interpose on its entry and exit so that we do
not change how developers write application code.

 \name{} requires a specific input payload schema in JSON
(Figure~\ref{fig:input-format}). When a function is invoked, the input data is
first received by the runtime ingress. The ingress uses the \texttt{Data}
field to read the user function's input data. If the \texttt{Source} is
\texttt{http}, the input data is directly embedded in the \texttt{Value}
field. Otherwise, \name{} uses the pointers in \texttt{Value} to read the
input data from the intermediary data store.

After user function returns, the egress runtime gets its output and checks the
IR configuration to see if there is a transition to execute. If no, it simply
terminates. If yes, it checks if the \texttt{Conditional} evaluates to true
and then executes the transition's egress.

\name{} keeps all runtime metadata in the input payload such that the
functions remain stateless and do not need to persist data via a data store
across invocations. \name{} uses the \texttt{Fan-out} field to store branch
indexes. The \texttt{Fan-out} field contains a recursive \texttt{OuterLoop}
field that \name{} uses to support nest fan-outs.

The runtime additionally uses a \texttt{Session} field to support concurrent
invocations of the same workflow. The \texttt{Session} field is a UUID string
that is unique to a workflow invocation and shared by all constituent function
instances in the invocation. Function checkpoint names are prefixed by the
\texttt{Session} string so that concurrently invocations do not overwrite each
other's data.

The \name{} runtime also implements exactly-once semantics with a
checkpointing mechanism which we discuss in the next section.


\section{Execution Guarantees}

An important characteristic of any workflow system 
is (a) how it deals with  a transient failure in a constituent step, and 
(b) what guarantees it makes in the presence of such faults.
 
Most workflow systems use checkpointing to limit the scope of re-execution after faults. 
In \name{}, each function is checkpointed to storage after it completes. In particular, if a
workflow experiences crashes mid-execution, \name{} does not retry from the beginning but
from the node of failure only.

\name{} guarantees \textbf{exactly-once execution}, meaning that each step of the 
workflow appears to execute exactly once. Even though FaaS engines already support automatic retries for functions,
they only guarantee at-least-once execution. Strengthening this guarantee requires dealing with the following subtleties:
\begin{itemize}
\item Function executions are not always deterministic, each re-execution may produce a different result.
\item The FaaS engine may detect failures incorrectly, thus multiple executions of a function can be in progress simultaneously.
\end{itemize}

Fortunately, we found a way to handle these challenges by taking advantage of conditional store operations 
supported by strongly consistent data stores. 
Specifically, \name{} guarantees that even if there are
multiple function execution instances, concurrent or not, only one instance's result is
taken as the final result and propagates to the downstream ingress node(s).
Other instances simply discard their results and terminate. 

\paragraph{Checkpoints and Synchronization.}
%\name{} uses a similar checkpointing technique across all transition patterns. 
After user code completes, the \name{} egress gadget immediately writes a checkpoint file that contains the user code
results to the intermediary data store. The checkpoint is uniquely named with
the instance's name (i.e., the name according the
\name{}~IR's naming scheme (\S\ref{sec:ir:naming}), prefixed by the workflow
invocation's unique session ID) such that the existence of a checkpoint
implies the corresponding function has successfully completed its user
function. The create operation is a conditional write and only succeeds when
the file does not already exist. If there are concurrent duplicate instances,
only one of them will create the checkpoint. The others will receive an error
from the write operation and \name{} runtime will simply terminate the
instance. The instance that successfully creates a checkpoint will proceeds to
executing its egress node and propagate its result to downstream functions.

For nonconcurrent duplicates (e.g., retries), \name{} checks if a checkpoint
exists \emph{before} running its user code. If a checkpoint does not exist,
\name{} goes ahead and runs the user code. Otherwise, \name{} reads the data
from the checkpoint and use that as final result without running user code
again. Then \name{} will run the ingress node to invoke the downstream function.
This is necessary because the duplicate might be a retry whose prior execution
crashed after checkpointing but before running the downstream ingress node. \name{} can
tolerate running a ingress/egress node more than once because of the same protection
against duplicates.

\paragraph{External Side Effects.}
In the applications targeted by \name{}, there is no need for functions to directly call external storage services. 
Rather, all functions are "side-effect-free": their only effect is to provide a result.
Therefore, the "exactly-one-result" guarantee of the \name{} runtime is in fact synonymous with exactly-once execution.

Although it is not currently required by any of our applications, it is conceivable to extend \name{} in the future to support
workflows whose functions have additional side effects. For example, a store interposition mechanism (such as used in Beldi \cite{beldi}) could 
be added to support functions that require more complex storage interactions than obtaining an input and producing a result.


\amit{TODO: Are there special considerations for exactly-once
	semantics? i.e. is checkpointing different than it is in chaining?}
\dhl{No. In fact checkpoint works independently from the control-flow
	patterns. The algorithm for exactly-once semantics is the same across all
	gadgets.
	
	Now specifically for fan-out it works like this: the fan-out initiator node
	will checkpoint right after user code returns which saves the data that is
	about to be fanned out. After checkpointing completes, the fan-out initiator
	node invokes the branches. If it crashes at any point during the series of
	invocations, unum retries the fan-out initiator lambda. The unum runtime on
	the lambda will see that a checkpoint with its name already exists, and
	therefore skips running the user function. But it will not skip retrying the
	fan-out, and it restarts the fan-out \emph{from the beginning}, which will
	result in duplicate instances for some or even all of the branches. But that
	is OK. Because the unum runtime protects against duplicates and still ensures
	exactly-once semantics. If the duplicates are concurrent (i.e., the original
	branch instances are still running), we protects that with a conditional write
	when checkpointing the results so only one instance of the duplicates will
	win. If the duplicates are nonconcurrent (i.e., the original branch instances
	already completed), the duplicates will skip running the user functions
	altogether.
	
	The fan-out initiator will keep retrying until a full fan-out is performed to
	make sure at-least-once invocation.
	
	% (explaining this makes me appreciate the consistency papers even more, 'cuz
	% this stuff is hard to explain.)
	
	So the patterns do not change the algorithms with which we provide exactly
	once. They work independently from each other. The only real gotcha we need to
	be careful with in \emph{implementing} exactly-once is nonidempotent
	operations, as pointed out by you. Specifically, the synchronizations across
	branches have to be idempotent. The purpose is prevent pre-mature fan-in.
	
}