\section{Design}\label{sec:design}

\name{} orchestrates the execution of workflows a decentralized manner on top of
a FaaS platform and a scalable consistent storage service. Workflows are modeled
as a directed execution graph where nodes represent user-defined FaaS functions
and each edge represents an invocation of one functions (incoming edge) with the
output of one or more other functions (outgoing edges). The FaaS platform
executes each function invocation at least once and may execute it more than
once due to lost messages, infrastructure failure, inconsistency in invocation
queues, etc.

An \name{} graph may can include fan-outs, where output from a node is used to
invoke several functions or split up and ``mapped'' multiple times on the same
function. Each such branch may be taken conditionally, based on the output value
or dynamic states of the graph. Execution graphs may also contain fan-ins, where
the outputs of multiple nodes are used to invoke a single aggregate function.
Cycles are also supported and each iteration through a cycle is a different
invocation of the target function.

\begin{table}[]
  \centering
  \begin{tabular}{|m{0.2\linewidth}|m{0.75\linewidth}|}
    \hline
	\textbf{Workflow} & A directed graph of functions that takes an input and produces one or more outputs. \\
    \hline
	\textbf{Function} & A user-defined FaaS function, linked with the \name{} runtime library. \\
    \hline
	\textbf{Invocation} & An invocation of a function one request of the FaaS platform to run a function and is associated with an argument to the function. \\
    \hline
	\textbf{Execution} & The FaaS platform may attempt to \emph{execute} a function invocation one or more times, and guarantees that at least one execution completes. \\
    \hline
  \end{tabular}
  \caption{\name{} terminology.}
  \label{table:terms}
\end{table}

\subsection{Architecture}\label{sec:design:architecture}

\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.8\textwidth}
	\centering
		\includegraphics[width=0.8\columnwidth]{figures/unum-arch-compile-time.pdf}
		% \includegraphics[width=\columnwidth]{figures/architecture.png}
		\caption{Serverless workflows form directed graphs. \name{}
		partitions the graph into an intermediate representation where each
		function is embedded with an \name{} configuration that encodes how to
		transition to its immediate downstream nodes. Developers package user
		function, \name{} config and \name{}'s runtime library (a pair of
		ingress and egress components) together to create unumized functions.}
		\label{fig:arch:unum-compile-time}

	\end{subfigure}
	\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=0.8\columnwidth]{figures/unum-arch-centralized.pdf}
		\caption{A typical serverless workflow system drives workflow logic
			using a centralized orchestrator that invokes constituent
			functions and waits for their outputs.}
		\label{fig:arch:centralized}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=.7\columnwidth]{figures/unum-arch-runtime.pdf}
		\caption{At runtime, \name{} orchestration logic is decentralized and
			runs in-situ with the user functions on an unmodified serverless
			platform. For synchronization and checkpointing,
			\name{} relies exclusively on a standard datastore of choice, such
			as DynamoDB or Cosmos DB.}
		\label{fig:arch:unum-runtime}
	\end{subfigure}
	\caption{\name{}'s Decentralized Orchestration. \name{} partitions
	orchestration logic at compile time and a \name{} runtime runs in-situ
	with user functions to perform only the orchestration logic local to its
	subsection of the graph.}
	\label{fig:arch}
\end{figure*}

Figure~\ref{fig:arch:unum-compile-time} depicts how a developer goes from a high
level workflow description and functions to running the workflow in a
decentralized manner using \name{}.

Developers write individual functions and describe the workflow using a
high-level workflow language, such as Step Function's expression language. They
pass this description and functions through a front-end \name{} compiler that
extracts portable \name{} IR for each node in the graph and ``attaches'' it to
the function (e.g.\ by placing a file containing the IR alongside the function
code). A backend \name{} linker ``links'' each function with a
platform-specific \name{} runtime library.~\footnote{Since functions are
typically written in dynamic languages, the \name{} library source code is
placed alongside the function and dynamically imported, rather than statically
linking an object file}

Runtime libraries are specific to the FaaS platform and datastore---e.g.\ Amazon
Lambda and DynamoDB, or Google Cloud Functions and Firestore. Each runtime is
composed of an ingress and egress component that run, respectively, before and
after the user-defined function. The egress component coalesces input data from
each incoming edge (e.g.\ in a fan-in), resolves input data if passed by name
rather than by value, and passes the input value to the function. The egress
component uses the function's result to invoke the next function(s) as
determined by the node's IR, ensures execution semantics using checkpoints,
performs coordination with sibling branches in fan-in, and deletes intermediate
states no longer needed for the workflow.

Developers deploy each linked function along with its IR to the FaaS platform.
The workflow is invoked by invoking the first function in the graph.  The
\name{} runtime is responsible for interpreting the \name{} IR at each node and
invoking next functions, performing coordination, checkpointing, and garbage
collection to execute the workflow in-situ with functions, in lieu of a
centralized orchestrator (Figure~\ref{fig:arch:unum-runtime}).

\subsection{\name{} Intermediate Representation}\label{sec:design:ir}


\begin{figure}[t!]
    \centering
    \begin{minted}[
        frame=single,
        fontsize=\scriptsize
        ]{rust}
type NodeName String
type InvocationName
    (SessionId, NodeName, Iteration, Vec<FanOut>)

struct IR {
  name: NodeName,
  next: Vec<Conditional<Invoke>>
}

enum Invoke {
  Scalar(NodeName),
  Map(NodeName),
  FanIn(NodeName, Vec<InvocationName>)
}

type Conditional<T> Fn(Input, Output) -> Option<T>
    \end{minted}
    \caption{\name{} IR}
    \label{fig:irschema}
\end{figure}

\section{Design}

To achieve the objectives in \S\ref{sec:goals}, \name{} utilizes a strategy we
call ``\emph{decentralized orchestration}'' where instead of executing
workflow orchestration logic entirely within a centralized orchestrator, a set
of ``decentralized orchestrators'' run \emph{in-situ} with user functions and
each performs only the orchestration logic \emph{local to its subsection} of
the workflow.

Efficiently implementing decentralized orchestration while also preserving the
benefits of centralized orchestrators (\S\ref{sec:bg:orchestrator}) requires
\name{} to solve three key challenges:

\squishenum
	\item How to partition its orchestration logic such that it can run in a
	decentralized manner in-situ with user functions given a high-level workflow description?

	\item How to efficiently execute orchestration logic in a
	decentralized manner, esp. when it requires data sharing and
	synchronization across function instances (e.g., fan-in)?

	\item How to provide exactly-once semantics when the
	orchestration logic is decentralized across function instances that may
	crash or retry mid-execution?
\squishenumend

\name{} contributes an end-to-end system that solves the three challenges and
delivers significant cost savings and improved or comparable performance than
a state-of-the-art production orchestrator (\S\ref{sec:eval}).
Figure~\ref{fig:arch} depicts \name{}'s architecture. \shadi{we should reference a,b,c individually as some later parts in the text.  btw, isn't 1.a the architecture rather than 1?}

\name{} solves the first challenge \emph{at compile time}, using a frontend
compiler and an intermediate representation (IR). Given a workflow definition
written in a high-level description language, the frontend compiler derives a
directed graph representation where nodes are user functions and edges
represent workflow transitions between functions. Based on the directed graph,
the compiler generates an IR in the form of configuration files, one file for
each node in the graph. A \textit{\name{} configuration} encodes all the outgoing edges
of a node such that each function in the workflow knows how to transition just
to its immediate downstream node(s).

\name{} solves the second challenge using an \textit{\name{} runtime} library
that efficiently implements a set of workflow patterns in a decentralized manner
and can run in-situ with user functions. In particular, the \name{} runtime is
made up of an ingress component and an egress component.  Developers package
each user function with its assigned \name{} configuration and the \name{}
runtime to create an \emph{unumized} function. When an unumized function
executes, its entry point is no longer the user code but instead the \name{}
ingress. And when user code completes, it returns its results to the \name{}
egress which then interprets its co-located \name{} configuration and performs
the workflow transition.

A critical complexity is to enable data sharing and synchronization across
functions in a workflow invocation. For example, applications might aggregate
the results of multiple upstream functions with a single ``sink'' function
when all of the upstream functions complete (commonly called a ``fan-in''). To
this end, \name{} leverages a shared \emph{intermediary data store} that is a
strongly consistent serverless storage (e.g., DynamoDB). Upstream functions
each write their output into the intermediary data store and use this data
store to share data and  synchronize across each other.


\name{} also leverages the intermediary data store to solve the third
challenge and provide a strong exactly-once semantics. \name{} uses a
checkpointing mechansim to limit the scope of retries when workflows crash
mid-execution and guarantee that even if there are multiple instances of the
same function, concurrent or not, only one instance's output is taken as the
final result and propargates downstream. When user functions do not produce
externally visible side-effects, \name{}'s exactly-once guarantee appears the
same as exactly-once semantics.

In the rest of the section, we first highlight a set of common workflow
patterns that \name{} supports (\S~\ref{sec:transition-patterns}), then
describe how the \name{} runtime efficiently executes workflows in a
decentralized manner (\S~\ref{sec:runtime}). Next, we show how the \name{} IR
encodes transitions between functions (\S\ref{sec:ir}) and allows partitioning
of workflows written as a high-level description. Finally, we detail \name{}'s
checkpointing mechanism and how it ensures strong execution guarantee
(\S~\ref{sec:exec-gntee}).


\input{patterns}
\input{runtime}
\input{ir-design}





\subsection{Execution Guarantees}\label{sec:exec-gntee}

An important characteristic of any workflow system is (a) how it deals with  a
transient failure in a constituent step, and (b) what guarantees it makes in
the presence of such faults.
 
Workflow systems typically persist progress to limit the scope of re-execution after faults
\cite{aws-step-functions, durable-functions, netherite, google-workflows, kappa}.
Likewise, \name{} checkpoints each function to storage.
 In particular, if a workflow experiences crashes mid-execution,
\name{} does not retry from the beginning but from the node of failure only.

In terms of progress and consistency, \name{} guarantees \textbf{exactly-once semantics}, 
meaning that the system records
exactly one result for each step of the workflow. FaaS engines already
support automatic retries for functions; this helps to ensure progress, as transient
faults do not compromise progress. However, we still need to
strengthen this at-least-once guarantee to an exactly-once guarantee, which 
is nontrivial because of the following subtleties:

\squishlist
	\item Function executions are not always deterministic, each re-execution
	may produce a different result.
	\item Some FaaS engines may detect failures incorrectly, thus multiple
	executions of a function can be in progress simultaneously, and may all run to completion.
\squishend
\vspace{1ex}

Fortunately, we found a way to handle these challenges by taking advantage of
conditional store operations supported by strongly consistent data stores.
Specifically, \name{} guarantees that even if there are multiple function
execution instances, concurrent or not, only one instance's result is taken as
the final result and propagates to the downstream ingress node(s). Other
instances simply discard their results and terminate.

\paragraph{Checkpoints and Synchronization.}%\name{} uses a similar checkpointing technique across all transition patterns. 
After user code completes, the \name{} egress immediately writes a checkpoint
file that contains the user code results to the intermediary data store. The
checkpoint is uniquely named with the instance's name (i.e., the name
according the
\name{}~IR's naming scheme (\S\ref{sec:ir:naming}), prefixed by the workflow
invocation's unique session ID) such that the existence of a checkpoint
implies the corresponding function has successfully completed its user
function. The create operation is a conditional write and only succeeds when
the file does not already exist. If there are concurrent duplicate instances,
only one of them will create the checkpoint. The others will receive an error
from the write operation and \name{} runtime will simply terminate the
instance. The instance that successfully creates a checkpoint will proceeds to
executing its egress node and propagate its result to downstream functions.

For nonconcurrent duplicates (e.g., retries), \name{} checks if a checkpoint
exists \emph{before} running its user code. If a checkpoint does not exist,
\name{} goes ahead and runs the user code. Otherwise, \name{} reads the data
from the checkpoint and use that as final result without running user code
again. Then \name{} will run the ingress node to invoke the downstream function.
This is necessary because the duplicate might be a retry whose prior execution
crashed after checkpointing but before running the downstream ingress node. \name{} can
tolerate running a ingress/egress node more than once because of the same protection
against duplicates.

\paragraph{External Side Effects.} Naturally, the
exactly-once guarantee does not automatically extend to functions with
external side effects, i.e. functions that directly call external services. In
such cases, retries can lead to unexpected results if the effects are not
idempotent. This issue is well known, and independent of the orchestrator
architecture (centralized vs. decentralized). Thus, we consider the question
of how to control such side effects to be orthogonal and beyond the scope of
this paper. For example, a store interposition libary like Beldi \cite{beldi}
can solve this problem.
