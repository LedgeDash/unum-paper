\section{Design}

\begin{figure*}[t]
	\centering
	\begin{subfigure}[t]{0.8\textwidth}
	\centering
		       \includegraphics[width=0.8\columnwidth]{figures/unum-arch-compile-time.pdf}
		% \includegraphics[width=\columnwidth]{figures/architecture.png}
		\caption{Serverless workflows form directed graphs. \name{}
		partitions the graph into an intermediate representation where each
		function is embedded with a configuration that encodes how to
		transition to its immediate downstream nodes. Developers package user
		function, \name{} config and \name{}'s runtime library (\deorc{})
		together to create unumized functions.}
		\label{fig:arch:unum-compile-time}

	\end{subfigure}
	\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=0.8\columnwidth]{figures/unum-arch-centralized.pdf}
		\caption{A typical serverless workflow system drives workflow logic
			using a centralized orchestrator that invokes constituent
			functions and waits for their outputs.}
		\label{fig:arch:centralized}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=.7\columnwidth]{figures/unum-arch-runtime.pdf}
		\caption{At runtime, \name{} orchestration logic is decentralized and
			runs in-situ with the user functions on an unmodified serverless
			platform. For synchronization and checkpointing,
			\name{} relies exclusively on a standard datastore of choice, such
			as DynamoDB or Cosmos DB.}
		\label{fig:arch:unum-runtime}
	\end{subfigure}
	\caption{\name{}'s Decentralized Orchestration. \name{} partitions
	orchestration logic at compile time and a decentralized orchestrator
	(\deorc{}) runs in-situ with user functions to perform only the
	orchestration logic local to its subsection of the graph.}
	\label{fig:arch}
\end{figure*}

To achieve the objectives in \S\ref{sec:bg}, \name{} utilizes a
design/strategy we call ``\emph{decentralized orchestration}'' where instead
of executing workflow orchestration logic entirely within a centralized
orchestrator, a set of ``decentralized orchestrators'' run \emph{in-situ} with
user functions and each performs only the orchestration logic \emph{local to
its subsection} of the workflow.

Efficiently implementing decentralized orchestration while also preserving the
benefits of centralized orchestrators (\S\ref{sec:bg:orchestrator}) requires
\name{} to solve three key challenges:

\squishenum
	\item Given a workflow written in a high-level description language, how
	to partition its orchestration logic such that it can run in a
	decentralized manner in-situ with user functions?

	\item How to efficiently execute orchestration logic in a
	decentralized manner, esp. when it requiers data sharing and
	synchronization across function instances (e.g., fan-in)?

	\item How to provide exactly-once execution semantics when the
	orchestration logic is decentralized across function instances that can
	crash and retry at any point mid-execution?
\squishenumend

\name{} contributes an end-to-end system that solves the three challenges and
delivers significant cost savings and improved or comparable performance than
a state-of-the-art production orchestrator (\S\ref{sec:eval}).
Figure~\ref{fig:arch} depicts
\name{}'s architecture. \shadi{the contrast part is not very clear. is it
essentially a vs b?} \dhl{Can you elaborate on \emph{why} it is not clear? a
is compile time. b and c are both runtime.}

\name{} solves the first challenge \emph{at compile time}, using a frontend
compiler and an intermediate representation (IR). Given a workflow definition
written in a high-level description language, the frontend compiler derives a
directed graph representation where nodes are user functions and edges
represent workflow transitions between functions. Based on the directed graph,
the compiler generates an IR in the form of configuration files, one file for
each node in the graph. A \name{} configuration encodes all the outgoing edges
of a node such that each function in the workflow knows how to transition just
to its immediate downstream nodes.

With the configurations from the IR, \name{} solves the second challenge using
a decentralized orchestrator (\deorc) \shadi{hard name. also, you don't have an orchestrator anymore, is this the best term? unum runtime, unum orchestrator, or unum-orch. is better in my opinion if so, at least use "deOrc" or "de-orc".} \shadi{a cosmetics point: using the texttt for so many keywords makes it hard to read. I would only use for code, function names, etc. not for unum or deorc.} that efficiently implements a set of
\seb{I agree with Shadi, would perhaps prefer avoiding "orchestrator" since we are doing "orchestration without an orchestrator". Many terms could work, e.g. ingress/egress gadget, continuation link, control flow connector, ... }
workflow patterns in a decentralized manner and can run in-situ with the
constituent user functions. In particular, the \deorc{} is a platform-specific
runtime library made up of an ingress component and a egress component.
Developers package each user function with its assigned \name{} configuration
and the \deorc{} to create an \emph{unumized} function. When an unumized
function executes, its entry point is no longer the user code but instead the
\deorc{} ingress. And when user code completes, it returns its results to the
\deorc{} egress which then interprets its co-located \name{} configuration and
performs the workflow transition.

A critical complexity is to enable fan-in patterns where a single ``sink''
function aggregates the results of multiple upstream functions. Importantly,
to avoid idle-billing (\S\ref{sec:bg}), we want the entire fan-in pattern to
execute \emph{wait-free}, i.e., to invoke the sink function only when all its
input upstream functions have completed. To this end, \name{} leverages a
shared \emph{intermediary data store} that is a strongly consistent \shadi{why? no motivation given} serverless
storage (e.g., DynamoDB) with conditional store operations \shadi{why? no motivation given}. Upstream functions
each writes user code output into and synchronizes over the intermediary data
store such that faster functions simply terminate and only the last-to-finish
function invokes the sink function. \shadi{I would not talk about fan-in here. you have not mentioned why we need fan-in, it comes out of the blue.}

\name{} leverages again the intermediary data store to also solve the third
challenge and provide \emph{exactly-once} execution semantics. \name{} uses
checkpointing, where each function checkpoints its output to the intermediary
data store, to limit the scope of retries when workflows crash mid-execution
and guarantee that even if there are multiple instances of the same function,
concurrent or not, only one instance's output is taken as the final result and
propargates downstream. \shadi{do we need to describe this here? if you move fan-in the data store is a bit out of the blue. it can work, but you can also just give fwd pointer in the 3 challenges at the beginning of the section.}

In the rest of the section, we first describe how the \name{} IR encodes
transitions between functions (\S\ref{sec:ir}), then explains how \deorc{}
efficiently executes transitions in a decentralized manner, and finally we
detail \name{}'s checkpointing mechanism and how it ensure exactly-once
execution semantics. \shadi{do we want to have Ir first or second?}

\input{patterns}
\input{ir-design}



\subsection{Decentralized Orchestration with \deorc{}}\label{sec:runtime}

\begin{figure}[t!]
	\centering
	\scalebox{1}{\includegraphics[width=\columnwidth]{figures/deorc-patterns.pdf}}
	\caption{\deorc{} implements transitions in a decentralized manner where
the egress on the source function(s) and ingress on the target function(s)
work together to execute the orchestration. \shadi{does this figure add value given you have Fig 1. you can probably combine the patterns there ans name them there and reuse that figure.}}
	\label{fig:transition}
\end{figure}

\begin{figure}[]
    \begin{minted}[
    frame=single,
    fontsize=\scriptsize
  ]{json}
{
    "Data": {
        "Source": "http | dynamodb | s3 | ...",
        "Value": "<object> | [<pointers>]"
    },
    "Session": "uuid",
    "Fan-out": {
        "Index": "int",
        "Size": "int",
        "OuterLoop": {
            "Index": "int",
            "Size": "int"
        }
    }
}
    \end{minted}
    \caption{\name{} runtime input payload schema}
    \label{fig:input-format}
\end{figure}

The second challenge of \name{} is to efficiently implement decentralized
orchestration. \name{} designs a decentralized orchestrator (\deorc) that
consists of an ingress and an egress component and can run in-situ with
constituent user functions. \deorc{} transparently interposes on user code
entry and exit so that it does not change how developers write application
code.

The primary purpose of \deorc{} is to interpret IR configurations and implement
transitions with platform-specific APIs. In particular, \deorc{} implements
transitions in a decentralized manner where the egress on the source
function(s) and ingress on the target function(s) work together to execute the
transition.

A key requirement of \deorc's design is to only use the basic serverless
abstraction without relying on any specialized APIs. Therefore, we designed
\deorc{} such that it only depends on two serverless components that are
universally supported by all platforms: i. a FaaS system that supports
asynchronous invocation of functions (e.g., AWS Lambda, Azure Functions,
Google Cloud Funtions, Openwhisk) and ii. a strongly consistent data store
that supports conditional write operations (e.g., DynamoDB, Cosmos DB).

Figure~\ref{fig:transition} depicts how \deorc{} executes \texttt{chain},
\texttt{map} and \texttt{fan-in}. The other patterns described in
\S\ref{sec:ir} are variations of these three patterns.

\paragraph{chain} 

The \texttt{chain} pattern involves one egress on source function and one
ingress on the target function. When the \texttt{Next} field of the source
function's \name{} config contains a single object whose \texttt{InputType} is
\texttt{Scalar}, the \deorc{} egress simply invokes the target function with
the source function's user code's output. \deorc{} uses asynchronous
invocation to avoid waiting and idle-billing and a particular input payload
schema in JSON (Figure~\ref{fig:input-format}) that contains \name{} runtime
metadata.

When the target function is invoked, the input is first received by the \deorc{}
ingress. The ingress uses the \texttt{Data} field to read the user function's
input data. If the \texttt{Source} is \texttt{http}, the input data is
directly embedded in the \texttt{Value} field. Otherwise,
\name{} uses the pointers in \texttt{Value} to read the input data from the
intermediary data store. Finally, ingress calls its user function and passes
it the input.

\paragraph{fan-out}

The \texttt{fan-out} pattern involves one egress on the source function and
many ingresses on the target functions.

Similar to chaining, the egress asynchronously invoke each target and the
ingresses on targets read the input data sent from the source and passes it to
its user function.

\texttt{map} is a simple variation of \texttt{fan-out} where the egress treats
its user code output as an iterable. \texttt{branch} is another variation
where the egress first evaluates the boolean condition in \texttt{Conditional}
before invoke the target function in that branch.

\paragraph{fan-in}

The \texttt{fan-in} pattern involves one ingress node and many egress nodes.
It is the main complexity of decentralized orchestration as we want to ensure
that the transition is \emph{wait-free} to avoid idle-billing. In particular,
we want to invoke the sink function only when all upstream functions have
completed so that the sink function does have to be spun up ahead of time and
wait for upstream functions to finish. Moreover, the upstream functions should
simply terminates when done instead of waiting for each other either.

To achieve this, the \deorc{} egress always writes the output of its user
function to a data store when its \name{} config has a \texttt{InputType} of
\texttt{Fan-in}. This serves two purposes: (1). it allows any of the upstream
functions to access the output of other upstream functions (2). it signals the
completion of a function. This way, each egress can simply writes its output
and terminate. Other egress nodes can still access completed egress' data
after they terminate. Any one of the egress can invoke the sink function. And
any one of the egress can see if other egress has completed or not.

Strongly consistent data store is important because it prevents the
scenarios where all egress have written outputs but none of them sees that all
have completed, which results in the sink function never invoked.

Additionally, \texttt{fan-in} makes sure that when the sink function is
invoked, it is invoked only once. \name{} achieves this by having the egress
nodes synchronize with each other via the same data store such that only the
last-to-finish egress invokes the sink function. Synchronization is done with
atomic read-after-write over a single object. Specific implementation depends
on the data store and we discuss the details in \S\ref{sec:impl}.

The last-to-finish egress invokes the sink function with a vector of pointers
to each upstream function's stored output. The pointers are the in same order
as the vector of upstream function names. The ingress on the sink function
dereferences each point by reading from the data store and passes a vector of
output values to its user function.


\subsubsection{Runtime Metadata}

To support a rich variety of orchestration patterns, \name{} requires a
specific input payload schema in JSON (Figure~\ref{fig:input-format}) that
contains \name{} runtime metadata. In particular, \name{} uses the
\texttt{Fan-out} field to store branch indexes. The \texttt{Fan-out} field
contains a recursive \texttt{OuterLoop} field that \name{} uses to support
nest fan-outs. The \texttt{\$0} and \texttt{\$size} variable in
\name{} IR (\S\ref{sec:ir}) refers to the \texttt{Index}  and \texttt{Size}
field of the top-level \texttt{Fan-out} field.

The runtime additionally uses a \texttt{Session} field to support concurrent
invocations of the same workflow. The \texttt{Session} field is a UUID string
that is unique to a workflow invocation and shared by all constituent function
instances in the invocation. Function checkpoint names
(\S\ref{sec:exec-gntee}) are prefixed by the \texttt{Session} string so that
concurrently invocations do not overwrite each other's data. We discuss
\name{} checkpoints and execution guarantees details in the next section.


\subsection{Execution Guarantees}\label{sec:exec-gntee}

An important characteristic of any workflow system is (a) how it deals with  a
transient failure in a constituent step, and (b) what guarantees it makes in
the presence of such faults.
 
Many workflow systems durably persist progress to limit the scope of re-execution after faults
\cite{aws-step-functions, durable-functions, netherite, google-workflows, kappa}.
In \name{}, each function is checkpointed to storage after it
completes. In particular, if a workflow experiences crashes mid-execution,
\name{} does not retry from the beginning but from the node of failure only.

\name{} guarantees \textbf{exactly-once execution}, meaning that each step of
the workflow appears to execute exactly once. FaaS engines already
support automatic retries for functions; however, they only guarantee at-least-once
execution. Strengthening this guarantee requires dealing with the following
subtleties:

\begin{itemize}
	\item Function executions are not always deterministic, each re-execution
	may produce a different result.
	\item Some FaaS engines may detect failures incorrectly, thus multiple
	executions of a function can be in progress simultaneously.
\end{itemize}

Fortunately, we found a way to handle these challenges by taking advantage of
conditional store operations supported by strongly consistent data stores.
Specifically, \name{} guarantees that even if there are multiple function
execution instances, concurrent or not, only one instance's result is taken as
the final result and propagates to the downstream ingress node(s). Other
instances simply discard their results and terminate.

\paragraph{Checkpoints and Synchronization.}%\name{} uses a similar checkpointing technique across all transition patterns. 
After user code completes, the \name{} egress gadget immediately writes a checkpoint file that contains the user code
results to the intermediary data store. The checkpoint is uniquely named with
the instance's name (i.e., the name according the
\name{}~IR's naming scheme (\S\ref{sec:ir:naming}), prefixed by the workflow
invocation's unique session ID) such that the existence of a checkpoint
implies the corresponding function has successfully completed its user
function. The create operation is a conditional write and only succeeds when
the file does not already exist. If there are concurrent duplicate instances,
only one of them will create the checkpoint. The others will receive an error
from the write operation and \name{} runtime will simply terminate the
instance. The instance that successfully creates a checkpoint will proceeds to
executing its egress node and propagate its result to downstream functions.

For nonconcurrent duplicates (e.g., retries), \name{} checks if a checkpoint
exists \emph{before} running its user code. If a checkpoint does not exist,
\name{} goes ahead and runs the user code. Otherwise, \name{} reads the data
from the checkpoint and use that as final result without running user code
again. Then \name{} will run the ingress node to invoke the downstream function.
This is necessary because the duplicate might be a retry whose prior execution
crashed after checkpointing but before running the downstream ingress node. \name{} can
tolerate running a ingress/egress node more than once because of the same protection
against duplicates.

\paragraph{External Side Effects.} In the applications targeted by \name{},
there is no need for functions to directly call external storage services.
Rather, all functions are "side-effect-free": their only effect is to provide
a result. 

%Therefore, the "exactly-one-result" guarantee of the \name{} runtime
%is in fact synonymous with exactly-once execution.

Although it is not currently required by any of our applications, it is
conceivable to extend \name{} in the future to support exactly-once execution even for workflows whose
functions have additional side effects. For example, a store interposition
mechanism (such as used in Beldi \cite{beldi}) could be added to support
functions that require more complex storage interactions than obtaining an
input and producing a result.