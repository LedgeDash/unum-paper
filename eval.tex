
\section{Evaluation}\label{sec:eval}



We evaluate unum along 3 metrics:

\begin{enumerate}
    
    \item Expressiveness

    \item Latency

    \item Costs

\end{enumerate}

% From application's perspective, the resource limitation is the number of
% parallel invocations that AWS allows for an account. Therefore, shorter
% end-to-end latency directly translates to higher throughput.

We evaluate the 3 metrics with a suite of micro- and macro-benchmarks. The
micro-benchmarks target specific operations (e.g., invoking a continuation,
checkpointing, etc.) and basic orchestration patterns (e.g., chaining, fan-out
and fan-in) to understand unum's performance characteristics and cost
benefits.

We then evaluate unum's expressiveness and end-to-end performance with 5
real-world applications, taken from serverless application repositories and
research literature. Each application covers a different use case and
orchestration pattern.

We compare unum with the Step Functions Standard workflows. We do not include
Step Functions Express workflows in our evaluation because of its poor
execution guarantee: a single invocation could produce multiple different
answers.



We show that [Key results and takeaways in 3 sentences.]

\subsection{Experimental setup}

Lambda, 128MB memory size unless otherwise specified.

On-demand DynamoDB as the intermediary data store

Step Functions Standard

All experiments run in the same Region: us-west-1 (cali.)

We try to avoid function cold starts whenever we can by pre-warming the
functions before experiments. However, because of Lambda's concurrency limit,
it is difficult to eliminate eviction and cold starts for large-scale
workloads (e.g., ExCamera).



\subsection{Microbenchmarks}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/TotalAdditionalLatency.pdf}
    \caption{Total latency unum incurs for a single state transition. We use a
    chain of two functions (\texttt{F->G}) that simply return their input.
    \texttt{data size} is the output data size of \texttt{F}, which in turn is
    the amount of data \texttt{F} writes to checkpoint and the input data size
    of \texttt{G}.}
    \label{fig:totallatency}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/OpLatency.pdf}
    \caption{Average latency incured by unum primitives. The bottom right
    figure shows the total latency of the one Lambda invoke call and two
    storage accesses as a percentage of total runtime overhead.}
    \label{fig:oplatency}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/OpLatency-pct.pdf}
    \caption{Latency breakdown of unum runtime. The majority of the latency is
    from the Lambda invoke call and the two storage accesses.}
    \label{fig:oplatency-pct}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/TotalCost.pdf}
    \caption{Total costs comparison of 1 million state transitions between
    Step Functions and \name{}}
    \label{fig:total-costs}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/MapMicroLatency.pdf}
    \caption{End-to-end latency (logscale) of parallel fan-out and fan-in.
    Lower is better. unum scales much better than the current version of Step
    Functions (experiments ran on Nov. 15, 2021) for parallel fan-out
    workloads. The result is not arguing that orchestrator-based workflow
    systems fundamentally scales worse than unum. In fact, the main cause of
    high latency in Step Functions is throttling of concurrent iterations when
    the fan-out size exceeds 40~\cite{aws-step-functions-map-state}. Although
    Step Functions does not elaborate on the reason for such throttling, there
    is little reason to believe that the maximum allowable concurrency is
    fundamentally capped around 40 for parallel workloads. However, this
    result does demonstrate an important downside of adding supplemental
    hosted services to support new workloads: any services added must also
    perform well compared with the highly scalable FaaS substrate and
    application developers must work with any restrictions that the services
    impose. unum avoids the additional work of building and maintaining hosted
    orchestrators and directly leverages the scalability of Lambda to achieve
    better parallel performance.}
    \label{fig:mapmicrolatency}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/ChainMicroLatency.pdf}
    \caption{End-to-end latency of chaining N functions. Lower is
    better. Each function simply returns the input data without performing any
    computation. Results in the figure are for 1KB of input data.}
    \label{fig:chainmicrolatency}
\end{figure}


\paragraph{How much additional latency does unum incur? How much does that
translate to costs?}

We first look at the performance and costs of the unum runtime. We measure the
latency of each primitive and the total additional latency for unum to take a
checkpoint and invoke a continuation. We translate the additional latency to
dollar costs in Lambda duration and compare it against Step Functions' costs
for state transitions. Note that unum taking a single checkpoint and invoking
a single continuation is equivalent to one Step Function state transition.




\subsection{Cost comparison}

Total cost of a state transition:

\begin{itemize}

    \item For Step Functions, a state transition incurs a fixed cost of \$27.8
    per 1M state transitions, regardless of the data size.

    \item For unum, there are two parts to the cost:

    \begin{enumerate}

        \item Lambda Duration costs from running the \name{} runtime. That is
        the additional time it takes to execute \name{} runtime code. This
        cost depends on the memory size of the Lambda. We'll assume the
        largest available Lambda size of 3072 MB which costs \$0.05 per 1M ms.

        \item Costs of checkpointing. Here we are mainly concerned with the
        cost of the read and write operations. We are \emph{not} concerned
        with the cost of storing checkpoints because the intermediary data can
        be delete right after workflow execution.

    \end{enumerate}

    \item S3 charges \$5.5 per 1M PUT requests (for checkpoint write) and \$0.44
    per 1M GET requests (for checkpoint read).

    \item DynamoDB on-demand capcity mode charges reads and writes based on
    the data size. 1M requests for writing 1KB data costs \$1.3942 and 1M
    requests for reading 1KB data costs \$0.279. Note that in the case of
    DynamoDB, if no faults happen during execution, the checkpoint read will
    return "item not found", which costs the same as returning 1KB of data.

    \item For 1M state transitions, \name{}'s costs for S3:

    \[  r(d)\times0.05 + 5.94 \]

    and for DynamoDB:

    \[  r(d)\times0.05 + d\times1.3942 +0.279\],

    where $r$ is the total additional runtime of \name{}, $d$ is the data
    size.

\end{itemize}


\paragraph{How well does unum perform with orchestration primitives (chaining,
fan-out, fan-in)?}


Though we want to compare the performance of basic workflow operations (e.g.,
invoking a function, checkpointing) between unum and Step Functions, Step
Functions logs do not provide sufficiently granular timestamps for an
apple-to-apple comparison. Therefore, we instead measure the end-to-end
latency of buidling-block orchestration patterns -- chaining, fan-out and
fan-in. To make sure the measurements mostly reflect the performance of the
workflow system and not the latency of user code, we minimize function
duration to 1ms so that the majority of latency comes from the workflow
system.




\paragraph{What's the cost of checkpointing? The total cost of the equivalence
of a single Step Functions state transition.}




% \subsection{Key Results}

% Invocation latency and invocation latency vs input payload size.

% Invocation latency translate to price,, i.e., 1 million invocations = \$ vs Step Function each state transition.

% Checkpoint latency (including the first check if a checkpoint exists) and
% checkpoint latency vs input payload size up to the Lambda invocation payload
% size limit. Because if you write Lambda applications, once your data is larger
% than the input payload size, you'd do your own data management anyway. This is
% perhaps not ideal API but this is not something that unum changes. In fact,
% it's not clear whether we should change this because some types of data are
% better when used with a data store.

% Total, invocation latency + checkpoint latency vs input payload size and
% translate this to \$ and compare with Step Functions for for instance 1million
% invocations.

% Do the same for a fan-out and fan-in:

% The fan-out initiator is going to spend XX ms to invoke all fan-out functions.
% This number should be roughly linear to <fan-out size> * individual invocation
% latency. So the invocation latency for a single continuation should be
% sufficient.

% Each fan-out function needs to spend XX extra ms to write to DynamoDB and the
% last to finish is going to invoke the fan-in function.

% The fan-in function needs to spend XX extra ms to read from the intermediary
% data store.


% % -------------

% Chaining latency vs chain length, compare with Step Functions

% Map Latency vs map size, compare with Step Functions

%     Example of an artificial limitations on scalability with additional service approach

% % -------------

% real applications/macrobenchmarks or case study?

% Not sure how to present the results. All applications in the same table? or
% each application with its own subsection? What if we end up with some
% applications that deserve its own subsection and some don't? Answer: start by
% writing each application with its ownd subsection, pour everything out and
% then decide how to organize.



% % --------------

% (Not sure how to discuss the expressiveness advantage or whether we should
% discuss at all)

% Step Functions no way to express a for loop or fold.

% Step Functions no way to express pipeline parallelism.

% Step Functions fan-in needs to make sure the aggregate data doesn't exceed a
% limit, unum automatically passes data in using pointers to the intermediary
% data store.
\subsection{Macrobenchmarks}

\subsubsection{Applications}

% Non-contrived

% Representative

% Describe what each application does, What pattern they use, what 


% \begin{table}[]
% \begin{tabular}{llllll}
% \hline
%                      &                        & \multicolumn{2}{l}{\textbf{unum}}                                                                                                   & \multicolumn{2}{l}{\textbf{Step Functions}}                                                                     \\
% \textbf{Application} & Pattern                & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                                                 & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                             \\ \hline
% IoT Pipeline         & chain                  & 120.9ms              & $0.2*2+(73+28)*$0.0021+2*\$1.3942                                                                            & 226.52               & $0.2*2+ 4*$27.9                                                                          \\
% Text Processing      & fan-out, fan-in        & 562.69ms             & $0.2*6+ (105+149+70+68+144+100)*$0.0021 + 6*$1.3942+2*2*$0.279                                               & 552.46ms             & $0.2*5+7*$27.9                                                                           \\
% Wordcount            & chain, fan-out, fan-in & 410s                 & $0.2*(1+262+1+250+1) + (277+6264*262 + 348 + 667*250 +68)*$0.0021 +(1+262+1+250+1)*$1.3942 + 262*2*$0.279+ 250*2* $0.279    & 898s                 & $0.2*(1+262+1+250+1) + (5913*262 + 154 + 633*250 +5)*$0.0021 +(1+262+1+1+250+1+1)*\$27.9 \\
% ExCamera             & chain, fan-out, fold   & 84s                  & $0.2*(1+16+15+15+14) + (6500+1500+350+4500+5000)*$0.0021+ (1+16+15+15+14)*$1.3942 + 15*2*$0.279+14*2*\$0.279 & 98s                  & $0.2*(16+16+1+16+15)+(6300+1400+2+5500+5300)*$0.0021+(1+16+16+1+1+16+1+1)*\$27.9      \\ \hline
% \end{tabular}
% \end{table}

\begin{table*}[t]
\begin{tabular}{llllll}
\hline
                     &                        & \multicolumn{2}{c}{\textbf{unum}}                                                                                                   & \multicolumn{2}{c}{\textbf{Step Functions}}                                                                     \\
\textbf{Application} & \textbf{Pattern}                & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                                                 & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                             \\ \hline
IoT Pipeline         & chain                  & 120.9ms              & \$3.4005                                                                            & 226.52               & \$112                                                                          \\
Text Processing      & fan-out, fan-in        & 562.69ms             & \$12.0168                                               & 552.46ms             & \$196.3                                                                           \\
Wordcount            & chain, fan-out, fan-in & 410s                 & \$4904.79   & 898s                 & \$18113 \\
ExCamera             & chain, fan-out, fold   & 84s                  & \$150.91 & 98s                  & \$1530      \\ \hline
\end{tabular}
\end{table*}

\subsection{Writing Notes}

\subsubsection{What does success look like?}

\begin{enumerate}

    \item Expressiveness. That you can build a wide range of realistic
     applications with unum.

    \item Latency performance. That unum is on par with Step Functions.

    \item Cost. That running applications on unum is cheaper than Step
     Functions.

\end{enumerate}

We can tune the phrasing based on how much we promise in the Intro, but the
main metrics are the above three.

\subsubsection{Questions}

\begin{enumerate}

    \item How do we evaluate and present execution guarantee? Anything to show
     to convince our reader that it's correct?

    \item How do we evaluate other benefits that stem from a simpler design
     (the fact that unum gets rid of the needs for a separate orchestrator
     service), such as resource management, required staffing and other
     hosting costs.

    \item Should we run experiments with S3 and present those numbers?

\end{enumerate}

