
\section{Evaluation}\label{sec:eval}

We evaluate \name{} along 3 metrics: (1). cost, (2). latency, and (3).
expressiveness, to answer the following questions:

\begin{enumerate}

    \item What is the cost of running applications on \name{} and how does it
    compare to existing serverless workflow systems? Specifically, how much
    additional costs does the \name{} runtime incur in Lambda duration
    billing? And how much does it cost to write and read checkpoints to and
    from the intermediary data store?

    \item What is the latency performance of representative applications on
    \name{}? And how does it compare to existing serverless workflow systems?

    \item How expressive is \name{}'s representation of serverless workflows
    (the \name{} IR)? Can one build complex, real-world applications with
    \name{}?

\end{enumerate}


We use a suite of micro- and macro-benchmarks. The micro-benchmarks target the
basic operations of \name{} (e.g., invoking a continuation, checkpointing,
etc.) and building-block orchestration patterns (e.g., chaining, fan-out and
fan-in) to understand unum's performance characteristics and cost benefits.

The macro-benchmarks consists of 4 real-world applications, taken from
serverless repositories and prior research work, aiming to
evaluate unum's expressiveness and end-to-end performance and costs.

We show that 

\begin{itemize}

    \item over 97\% of \name{}'s latency overhead comes from API calls to
    Lambda and data stores, which means the bulk of \name{}'s performance will
    automatically improve with the underlying platform (e.g., a faster Lambda
    or data store) without any modification to \name{} itself.

    % \item The additional Lambda duration billing for executing \name{} runtime
    % is negligible across all data sizes

    \item \name{} is slightly faster (11-28\%) in chaining performance and
    much faster in parallel fan-out and fan-in performance (up to 4.58x),
    especially at higher level of parallelism, than Step Functions.

    \item \name{} delivers more than one order-of-magnitude cost savings for
    almost all applications we evaluated, even when using the more expensive
    DynamoDB as the intermediary data store. The applications we use cover all
    orchestration patterns that Step Functions currently support.

    \item \name{} is able to express all orchestration patterns that Step
    Functions currently support. Additionally, with the ExCamera
    implementation, we demonstrate that \name{} can express fold or for loops
    and support pipeline parallelism, neither of which is expressible in Step
    Functions.

\end{itemize}

\subsection{Experimental setup}

We run all experiments on AWS, region \texttt{us-west-1} and costs numbers
reflect \texttt{us-west-1} pricing. We configure lambdas to 128MB memory size
unless otherwise specified and use on-demand capacity mode for DynamoDB. To
avoid function cold starts, we pre-warm functions by running the workflow a
few doze times before collecting measurement.

% S3 buckets all have Versioning turned on.

We compare against Step Functions' \emph{Standard} Workflows as the baseline.
Similar to \name{}, the Standard Workflows persists execution states on every
state transition (i.e., completing one function and starting the next
function), and always returns exactly one response for one workflow
invocation~\cite{aws-step-functions-exec-gntee}.

We do not consider Step Functions' Express Workflows in our comparison because
of its weaker execution guarantee, namely the same invocation could result in
multiple, potentially different results if any part of your workflow logic is
nonidempotent~\cite{aws-step-functions-exec-gntee}.

Note that even though Step Functions claims that the Standard Workflows
provides "exactly-once workflow
execution"~\cite{aws-step-functions-exec-gntee}, it is not clear whether it
implies exactly-once execution for component functions of the workflows. Our
interpretation is that the internal states of a standard workflow will appear
to execute exactly once, but component functions might not run exactly-once
due to failures and retries, which is identical to \name{}.

\subsection{Microbenchmarks}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/TotalAdditionalLatency.pdf}
    \caption{Total latency unum incurs for a single state transition. We use a
    chain of two functions (\texttt{F->G}) that simply return their input.
    \texttt{data size} is the output data size of \texttt{F}, which in turn is
    the amount of data \texttt{F} writes to checkpoint and the input data size
    of \texttt{G}.}
    \label{fig:totallatency}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/OpLatency.pdf}
    \caption{Average latency incured by unum primitives. The bottom right
    figure shows the total latency of the one Lambda invoke call and two
    storage accesses as a percentage of total runtime overhead.}
    \label{fig:oplatency}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/OpLatency-pct.pdf}
    \caption{Latency breakdown of unum runtime. The majority of the latency is
    from the Lambda invoke call and the two storage accesses.}
    \label{fig:oplatency-pct}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/TotalCost.pdf}
    \caption{Total costs comparison of 1 million state transitions between
    Step Functions and \name{}}
    \label{fig:total-costs}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/MapMicroLatency.pdf}
    \caption{End-to-end latency (logscale) of parallel fan-out and fan-in.
    Lower is better. unum scales much better than the current version of Step
    Functions (experiments ran on Nov. 15, 2021) for parallel fan-out
    workloads. The result is not arguing that orchestrator-based workflow
    systems fundamentally scales worse than unum. In fact, the main cause of
    high latency in Step Functions is throttling of concurrent iterations when
    the fan-out size exceeds 40~\cite{aws-step-functions-map-state}. Although
    Step Functions does not elaborate on the reason for such throttling, there
    is little reason to believe that the maximum allowable concurrency is
    fundamentally capped around 40 for parallel workloads. However, this
    result does demonstrate an important downside of adding supplemental
    hosted services to support new workloads: any services added must also
    perform well compared with the highly scalable FaaS substrate and
    application developers must work with any restrictions that the services
    impose. unum avoids the additional work of building and maintaining hosted
    orchestrators and directly leverages the scalability of Lambda to achieve
    better parallel performance.}
    \label{fig:mapmicrolatency}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/ChainMicroLatency.pdf}
    \caption{End-to-end latency of chaining N functions. Lower is
    better. Each function simply returns the input data without performing any
    computation. Results in the figure are for 1KB of input data.}
    \label{fig:chainmicrolatency}
\end{figure}


\paragraph{How much additional latency does unum incur? How much does that
translate to costs?}

We first look at the performance and costs of the unum runtime. We measure the
latency of each primitive and the total additional latency for unum to take a
checkpoint and invoke a continuation. We translate the additional latency to
dollar costs in Lambda duration and compare it against Step Functions' costs
for state transitions. Note that unum taking a single checkpoint and invoking
a single continuation is equivalent to one Step Function state transition.




\subsection{Cost comparison}

Total cost of a state transition:

\begin{itemize}

    \item For Step Functions, a state transition incurs a fixed cost of \$27.8
    per 1M state transitions, regardless of the data size.

    \item For unum, there are two parts to the cost:

    \begin{enumerate}

        \item Lambda Duration costs from running the \name{} runtime. That is
        the additional time it takes to execute \name{} runtime code. This
        cost depends on the memory size of the Lambda. We'll assume the
        largest available Lambda size of 3072 MB which costs \$0.05 per 1M ms.

        \item Costs of checkpointing. Here we are mainly concerned with the
        cost of the read and write operations. We are \emph{not} concerned
        with the cost of storing checkpoints because the intermediary data can
        be delete right after workflow execution.

    \end{enumerate}

    \item S3 charges \$5.5 per 1M PUT requests (for checkpoint write) and \$0.44
    per 1M GET requests (for checkpoint read).

    \item DynamoDB on-demand capcity mode charges reads and writes based on
    the data size. 1M requests for writing 1KB data costs \$1.3942 and 1M
    requests for reading 1KB data costs \$0.279. Note that in the case of
    DynamoDB, if no faults happen during execution, the checkpoint read will
    return "item not found", which costs the same as returning 1KB of data.

    \item For 1M state transitions, \name{}'s costs for S3:

    \[  r(d)\times0.05 + 5.94 \]

    and for DynamoDB:

    \[  r(d)\times0.05 + d\times1.3942 +0.279\],

    where $r$ is the total additional runtime of \name{}, $d$ is the data
    size.

\end{itemize}


\paragraph{How well does unum perform with orchestration primitives (chaining,
fan-out, fan-in)?}


Though we want to compare the performance of basic workflow operations (e.g.,
invoking a function, checkpointing) between unum and Step Functions, Step
Functions logs do not provide sufficiently granular timestamps for an
apple-to-apple comparison. Therefore, we instead measure the end-to-end
latency of buidling-block orchestration patterns -- chaining, fan-out and
fan-in. To make sure the measurements mostly reflect the performance of the
workflow system and not the latency of user code, we minimize function
duration to 1ms so that the majority of latency comes from the workflow
system.




\paragraph{What's the cost of checkpointing? The total cost of the equivalence
of a single Step Functions state transition.}




% \subsection{Key Results}

% Invocation latency and invocation latency vs input payload size.

% Invocation latency translate to price,, i.e., 1 million invocations = \$ vs Step Function each state transition.

% Checkpoint latency (including the first check if a checkpoint exists) and
% checkpoint latency vs input payload size up to the Lambda invocation payload
% size limit. Because if you write Lambda applications, once your data is larger
% than the input payload size, you'd do your own data management anyway. This is
% perhaps not ideal API but this is not something that unum changes. In fact,
% it's not clear whether we should change this because some types of data are
% better when used with a data store.

% Total, invocation latency + checkpoint latency vs input payload size and
% translate this to \$ and compare with Step Functions for for instance 1million
% invocations.

% Do the same for a fan-out and fan-in:

% The fan-out initiator is going to spend XX ms to invoke all fan-out functions.
% This number should be roughly linear to <fan-out size> * individual invocation
% latency. So the invocation latency for a single continuation should be
% sufficient.

% Each fan-out function needs to spend XX extra ms to write to DynamoDB and the
% last to finish is going to invoke the fan-in function.

% The fan-in function needs to spend XX extra ms to read from the intermediary
% data store.


% % -------------

% Chaining latency vs chain length, compare with Step Functions

% Map Latency vs map size, compare with Step Functions

%     Example of an artificial limitations on scalability with additional service approach

% % -------------

% real applications/macrobenchmarks or case study?

% Not sure how to present the results. All applications in the same table? or
% each application with its own subsection? What if we end up with some
% applications that deserve its own subsection and some don't? Answer: start by
% writing each application with its ownd subsection, pour everything out and
% then decide how to organize.



% % --------------

% (Not sure how to discuss the expressiveness advantage or whether we should
% discuss at all)

% Step Functions no way to express a for loop or fold.

% Step Functions no way to express pipeline parallelism.

% Step Functions fan-in needs to make sure the aggregate data doesn't exceed a
% limit, unum automatically passes data in using pointers to the intermediary
% data store.
\subsection{Macrobenchmarks}

\subsubsection{Applications}

% Non-contrived

% Representative

% Describe what each application does, What pattern they use, what 


% \begin{table}[]
% \begin{tabular}{llllll}
% \hline
%                      &                        & \multicolumn{2}{l}{\textbf{unum}}                                                                                                   & \multicolumn{2}{l}{\textbf{Step Functions}}                                                                     \\
% \textbf{Application} & Pattern                & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                                                 & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                             \\ \hline
% IoT Pipeline         & chain                  & 120.9ms              & $0.2*2+(73+28)*$0.0021+2*\$1.3942                                                                            & 226.52               & $0.2*2+ 4*$27.9                                                                          \\
% Text Processing      & fan-out, fan-in        & 562.69ms             & $0.2*6+ (105+149+70+68+144+100)*$0.0021 + 6*$1.3942+2*2*$0.279                                               & 552.46ms             & $0.2*5+7*$27.9                                                                           \\
% Wordcount            & chain, fan-out, fan-in & 410s                 & $0.2*(1+262+1+250+1) + (277+6264*262 + 348 + 667*250 +68)*$0.0021 +(1+262+1+250+1)*$1.3942 + 262*2*$0.279+ 250*2* $0.279    & 898s                 & $0.2*(1+262+1+250+1) + (5913*262 + 154 + 633*250 +5)*$0.0021 +(1+262+1+1+250+1+1)*\$27.9 \\
% ExCamera             & chain, fan-out, fold   & 84s                  & $0.2*(1+16+15+15+14) + (6500+1500+350+4500+5000)*$0.0021+ (1+16+15+15+14)*$1.3942 + 15*2*$0.279+14*2*\$0.279 & 98s                  & $0.2*(16+16+1+16+15)+(6300+1400+2+5500+5300)*$0.0021+(1+16+16+1+1+16+1+1)*\$27.9      \\ \hline
% \end{tabular}
% \end{table}

\begin{table*}[t]
\begin{tabular}{llllll}
\hline
                     &                        & \multicolumn{2}{c}{\textbf{unum}}                                                                                                   & \multicolumn{2}{c}{\textbf{Step Functions}}                                                                     \\
\textbf{Application} & \textbf{Pattern}                & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                                                 & \textit{e2e latency} & \textit{cost (per 1M exec.)}                                                             \\ \hline
IoT Pipeline         & chain                  & 120.9ms              & \$3.4005                                                                            & 226.52               & \$112                                                                          \\
Text Processing      & fan-out, fan-in        & 562.69ms             & \$12.0168                                               & 552.46ms             & \$196.3                                                                           \\
Wordcount            & chain, fan-out, fan-in & 410s                 & \$4904.79   & 898s                 & \$18113 \\
ExCamera             & chain, fan-out, fold   & 84s                  & \$150.91 & 98s                  & \$1530      \\ \hline
\end{tabular}
\end{table*}

\subsection{Writing Notes}

\subsubsection{What does success look like?}

\begin{enumerate}

    \item Expressiveness. That you can build a wide range of realistic
     applications with unum.

    \item Latency performance. That unum is on par with Step Functions.

    \item Cost. That running applications on unum is cheaper than Step
     Functions.

\end{enumerate}

We can tune the phrasing based on how much we promise in the Intro, but the
main metrics are the above three.

\subsubsection{Questions}

\begin{enumerate}

    \item How do we evaluate and present execution guarantee? Anything to show
     to convince our reader that it's correct?

    \item How do we evaluate other benefits that stem from a simpler design
     (the fact that unum gets rid of the needs for a separate orchestrator
     service), such as resource management, required staffing and other
     hosting costs? Further on the resource utilization point, do we want to
     say that dollar costs of running applications is a reasonable enough
     proxy to resource consumption and therefore lower price = less resource
     consumption = better resource utilization?

    \item Should we run experiments with S3 and present those numbers?

\end{enumerate}

