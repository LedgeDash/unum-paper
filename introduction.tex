\section{Introduction}

Serverless computing offers a simple and yet powerful abstraction consisting of
stateless compute component (Functions as a Service, or FaaS) and scalable,
multi-tenant data stores~\cite{berkeley}. Developers build applications using
task-specific, event-driven ``functions'' without the need to provision or
manage servers. In turn, serverless platforms have been able to offer high
burst-scalability and quickly relaim function resources as soon as they are
idle. Developers are charged in small compute and storage increments (100s of
milliseconds and per-stored-object and request, respectively), and platforms
can make efficient use of hardware resources.\amit{I think we should avoid the
BaaS acronym. Berkeley used it, but it's confusing. It means
``Backend-as-a-Service'' and in practice is used to refer to a wide variety of
non-storage-only services}

Originally, serverless platforms targeted simple applications with one or a few
functions and simple interaction patters. Recently, though, there has been
emerging interest using serverless platforms to build complex serverless
applications composed of many functions with rich interaction
patterns~\cite{excamera, pywren, gg-atc, beldi, boki}. Unforunately, building
such applications using basic serverless building blocks is challenging. FaaS
invocation semantics require applications to handle individual function
failures and retries gracefully, and drives complex control flows in a
decentralized manner is both difficult to implement correctly and difficult to
express.

\amit{Note from previous version of 2nd paragraph: As the second paragraph,
this implies that the main thing we're contributing is going to be a
higher-level interface and execution guarantees. But I think we've moved away
from that as the key contribution. Instead, I think the thing that these
workloads call for is ``stateful''ness (needs to be defined still)}
\dhl{I thought about using the term "statefulness", but decided
against it because (1). Beldi (OSDI '20) and Boki (SOSP '21) both specify
"statefulness" as the key differentiator \emph{but their definition is
different}. In their definition, "statefulness" mainly refers to the behavior
of individual functions that explicitly persists application data in DynamoDB.
Specifically, Beldi defines "stateful serverless functions (SSFs)" as
functions that "maintains their own state (e.g., modify a data structure that
persists across invocations)" (top of right column on page 1). It's a
definition that works well for their problem because they want consistent
write results in face of faults and retries (2). Not all our gadgets are
straight-forwardly stateful. Chaining for example doesn't "appear" stateful,
and \name{} runtime doesn't explicitly manage states for chaining. The only
pattern that requires \name{} to explicit manage states is fan-in.}

To meet this need, several workflow systems have emerged offering
higher-level programming interfaces for expressing function interactions, a
rich set of composition primitives that supports chaining, branching, fan-out
and fan-in and exactly-once semantics for workflow execution~\cite{excamera,
gg-atc, aws-step-functions, google-cloud-composer, google-workflows,
durable-functions}. Designs of existing solutions all require adding an
additional stateful component, often called an orchestrator or controller, to
current serverless infrastructures.

An orchestrator is a stateful hosted service, separate from FaaS and BaaS,
that centralizes states and control-flow. For each workflow invocation, an
orchestrator instance interprets the workflow definition and drives execution
by invoking constituent functions, waits for their results and pass them to
downstream functions. All function invocations are initiated by the
orchestrator and all workflow states (e.g., function results) flow through the
orchestrator.

While a solution that meets application requirements, orchestrators break out
of the serverless abstraction, neglecting and even compromising key serverless
benefits. First, the existing serverless infrastructure already provides
highly performant and scalable compute and storage components. Adding
orchestrators is repeating the expensive process of building and maintaining a
hosted service, which usually requires a dedicated engineering team.
Furthermore, application end-to-end performance now also depends on the
orchestrator. A slow orchestrator can become a bottleneck and nullify the fast
autoscale advantage of serverless. Lastly, orchestrators expose specialized
proprietary APIs. Applications written for an orchestrator have to work with
the propensities and limitations of the APIs and is locked in to a particular
provider. \dhl{The drawbacks of orchestrators. Ideas are here. But need better
wording.}

In this paper, we propose \name{}, a new serverless workflow system design
that preserves all the advantages of orchestrators but execute in a purely
serverless environment, without adding any supplemental components to current
infrastructures. We show that by building on top of the existing serverless
components that are already highly performant and scalable, \name{} removes
the expensive process of building and maintaining a separate hosted service
for serverless providers, significantly reduces costs by up to 32.8x for
serverless users, and improves application end-to-end latency by up to 4.5x,
especially applications with high degrees of parallelism.

\dhl{Design insights, highlights and overview}

% \name{} uses a two-stage compiler that tackles the complexity of orchestration
% at compile-time and removes the need to use hosted orchestrator services for
% workflow execution. The compiler transforms workflow definitions (e.g., Step
% Functions state machines) to a continuation-based intermediary representation
% (the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
% constiuent functions such that each function is responsible for invoking its
% immediate downstream functions.

% Workflow functions in \name{} are deployed with a runtime wrapper that
% transparently interposes on user code entry and exit. \name{} runtime wrapper
% does not change how users build functions. Developer can still write functions
% exactly the same way as if they are individual functions, and do not need to
% import any additional libraries in order to use \name{}.

% During execution, when user code completes, the runtime wrapper executes the
% assigned continuations which triggers its immediate downstream functions in
% the workflow. Each function performs the same action in the order defined in
% the workflow such that the orchestration logic is distributedly executed by
% the collection of constituent functions.\shadi{this last sentence is a bit vague, reword.}

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

\name{} provides a set of execution guarantees that serverless workflows care
about. Specifically, \name{} ensures at-least-once execution on constituent
function, which guarantees that workflow execution does not hang
mid-execution. If a workflow execution crashes, \name{} automatically retries
the function where the crash happens without restarting the worklfow from the
beginning. Furthermore, \name{} provides exactly-once semantics such that even
if a function executes multiple times, the final states appear as if it
executes once.

% We present and evaluate an implementation of \name{} that can compile Step
% Functions state machines and execute them purely as Lambda functions using
% either S3 or DynamoDB as the data store. The implementation supports all
% orchestration patterns in Step Functions. Our experimental results show that,
% compared with Step Functions, \name{} improves performance by 11-28\% in the
% case of chaining functions and up to 4.5x in high-parallelism patterns such as
% fan-out. At the same time, \name{} significantly reduces the cost of running
% applications by 3.7x to 32.8x.

This paper makes the following contributions:

\begin{itemize}

  \item An end-to-end system that can take workflows written in higher-level
  language and execute them purely as FaaS functions.

  \item A set of general algorithms, called gadgets, that implement all common
  orchestration patterns (e.g., chaining, branching, fan-out, fan-in) with
  serverless functions and serverless storage.

  \item An intermediate representation language that can express complex
  workloads using these gadgets in a platform agnostic way, and a front-end
  compiler that transforms Step Functions state machines to the IR.

  \item An implementation of the \name{} runtime that execute workflows purely
  as Lambda functions using either S3 or DynamoDB as the data store for enable
  fan-in and provide exactly-once semantics.

\end{itemize}
% -------

% execution guarantee challenges.
