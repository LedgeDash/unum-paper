\section{Introduction}

Serverless computing provides a simple and yet powerful abstraction that
consists of a stateless compute component (FaaS) and autoscaling,
pay-for-what-you-use data stores (BaaS)~\cite{berkeley}. Developers build
applications in the form of event-driven functions without the need to
provision and manage resources. Serverless platforms execute functions with
fast autoscaling that can start thousands of instances in seconds,
fine-grained billing that only charges for resources used, and improved
utilization that reclaim resources immediately when they become idle.

There are strong interests in building large serverless applications composed
of multiple functions with rich patterns of interactions~\cite{excamera,
pywren, gg-atc, beldi, boki}. In addition to building and executing individual
functions, these workloads call for a \emph{higher-level programming
interface} that expresses interactions between functions and \emph{strong
end-to-end guarantees} for correct executions.

Several serverless workflow systems have emerged to meet the demand, offering
higher-level programming interfaces for expressing function interactions, a
rich set of composition primitives that supports chaning, branching, fan-out
and fan-in and exactly-once semantics for workflow execution~\cite{excamera,
gg-atc, aws-step-functions, google-cloud-composer, google-workflows,
durable-functions}. Designs of existing solutions all require adding an
additional stateful component, often called an orchestrator or controller, to
the current serverless infrastructure. An orchestrator is a stateful hosted
service, separate from FaaS and BaaS. For each workflow invocation, an
orchestrator instance interprets the workflow definition and drives execution
by invoking constituent functions, waits for their results and pass them to
downstream functions. All function invocations are initiated by the
orchestrator and all workflow states (e.g., function results) flow through the
orchestrator.

While a solution that meets application requirements, orchestrators break out
of the serverless abstraction, neglecting and even compromising key serverless
benefits. First, the existing serverless infrastructure already provides
highly performant and scalable compute and storage components. Adding
orchestrators is repeating the expensive process of building and maintaining a
hosted service, which usually requires a dedicated engineering team.
Furthermore, application end-to-end performance now also depends on the
orchestrator. A slow orchestrator can become a bottleneck and nullify the fast
autoscale advantage of serverless. Lastly, orchestrators expose specialized
proprietary APIs. Applications written for an orchestrator have to work with
the propensities and limitations of the APIs and is locked in to a particular
provider.

In this paper, we propose \name{}, a new serverless workflow system design
that preserves all the advantages of orchestrators but execute in a purely
serverless environment, without adding any supplemental components to current
infrastructures. We show that by building on top of the existing serverless
components that are already highly performant and scalable, \name{} removes
the expensive process of building and maintaining a separate hosted service
for serverless providers, and significantly improves application end-to-end
latency by up to 4.5x, especially at high degrees of parallelism, and saves
3.7x to 32.8x on costs for serverless users.

\dhl{Design insights, highlights and overview}

% \name{} uses a two-stage compiler that tackles the complexity of orchestration
% at compile-time and removes the need to use hosted orchestrator services for
% workflow execution. The compiler transforms workflow definitions (e.g., Step
% Functions state machines) to a continuation-based intermediary representation
% (the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
% constiuent functions such that each function is responsible for invoking its
% immediate downstream functions.

% Workflow functions in \name{} are deployed with a runtime wrapper that
% transparently interposes on user code entry and exit. \name{} runtime wrapper
% does not change how users build functions. Developer can still write functions
% exactly the same way as if they are individual functions, and do not need to
% import any additional libraries in order to use \name{}.

% During execution, when user code completes, the runtime wrapper executes the
% assigned continuations which triggers its immediate downstream functions in
% the workflow. Each function performs the same action in the order defined in
% the workflow such that the orchestration logic is distributedly executed by
% the collection of constituent functions.\shadi{this last sentence is a bit vague, reword.}

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

\name{} provides a set of execution guarantees that serverless workflows care
about. Specifically, \name{} ensures at-least-once execution on constituent
function, which guarantees that workflow execution does not hang
mid-execution. If a workflow execution crashes, \name{} automatically retries
the function where the crash happens without restarting the worklfow from the
beginning. Furthermore, \name{} provides exactly-once semantics such that even
if a function executes multiple times, the final states appear as if it
executes once.

% We present and evaluate an implementation of \name{} that can compile Step
% Functions state machines and execute them purely as Lambda functions using
% either S3 or DynamoDB as the data store. The implementation supports all
% orchestration patterns in Step Functions. Our experimental results show that,
% compared with Step Functions, \name{} improves performance by 11-28\% in the
% case of chaining functions and up to 4.5x in high-parallelism patterns such as
% fan-out. At the same time, \name{} significantly reduces the cost of running
% applications by 3.7x to 32.8x.

This paper makes the following contributions:

\begin{itemize}

  \item A set of general algorithms, called gadgets, that implement all common
  orchestration patterns (e.g., chaining, branching, fan-out, fan-in) with
  serverless functions and serverless storage.

  \item An intermediate representation language that can express complex
  workloads using these gadgets in a platform agnostic way, and a front-end
  compiler that transforms Step Functions state machines to the IR.

  \item An implementation of the \name{} runtime that execute workflows purely
  as Lambda functions using either S3 or DynamoDB as the data store for enable
  fan-in and provide exactly-once semantics.

\end{itemize}
% -------

% execution guarantee challenges.
