\section{Introduction}

Serverless computing offers a simple but powerful abstraction consisting of
stateless compute component (Functions as a Service, or FaaS) and scalable,
multi-tenant data stores~\cite{berkeley}. Developers build applications using
task-specific, event-driven ``functions'' without the need to provision or
manage servers. In turn, serverless platforms have been able to offer high
burst-scalability and quickly reclaim function resources as soon as they are
idle. Developers are charged in small compute and storage increments (100s of
milliseconds and per-stored-object and request, respectively), and platforms
can make efficient use of hardware resources.

Originally, serverless platforms targeted simple applications with one or a few
functions and simple interaction patterns. Recently, though, there has been
emerging interest in using serverless platforms to build more complex
applications composed of 10s to 100s functions with rich interaction
patterns~\cite{excamera, pywren, gg-atc, beldi, boki}. Unfortunately, building
such applications using basic serverless building blocks is challenging.
Handling complex control flows and large numbers of functions, in an ad-hoc
manner is difficult to both express and implement correctly. Additionally, FaaS
invocation semantics requires application developers to gracefully handle
individual function failures and retries.

To meet this need, several workflow systems have emerged to enable serverless
applications with large and complex ``workflows'' of functions. These offer
high-level programming interfaces to express function interactions, supporting a
rich set of composition patterns for chaining, branching, fan-out and fan-in, as
well as exactly-once workflow execution semantics~\cite{excamera, gg-atc,
aws-step-functions, google-cloud-composer, google-workflows, durable-functions}.
These systems rely on some form of \emph{orchestrator} to track workflow
progress, implement interaction logic, and correctly handle retries of
individual steps due to faults or inconsistency in function invocation queues.

An orchestrator is a stateful component that centralizes execution states and
control-flow operations. All function invocations are initiated by the
orchestrator, and all workflow states (e.g., function results) flow through the
orchestrator. Therefore, building complex interaction patterns is relatively
straightforward: for each workflow transition in a serverless application, the
orchestrator waits for currently running functions to complete, determines what
to do with their results, and which function(s) to run next. Similarly, a
centralized orchestrator simplifies failure handling. The entire workflow state
resides in the orchestrator and can be easily checkpointed.

However, centralization is not a panacea. Centralized orchestrators are
communication-intensive, as all control and data must flow back and forth
between the orchestrator and the functions. Moreover, because orchestration
functionality is commonly provided by a separate stateful hosted service
\cite{aws-step-functions, google-cloud-composer, google-workflows}, this
requires development and operation of yet another scalable, multi-tenant,
fault-tolerant, billable service, in addition to serverless compute and storage
infrastructure.

%Such centralized orchestrators enable new classes of serverless applications,
%but requires an additional multi-tenant service to be maintained and operated
%and adds additional constraints on performance and scalability.

While basic serverless build blocks are the result of decades of research and
engineering to achieve performant and scalable compute and storage
infrastructure, orchestrators perform specialized functionality that must solve
scalability and performance problems independently. Applications may suffer, for
example, from scalability bottlenecks in the orchestrator even if storage and
FaaS platforms can support much higher burst-scalability. Moreover, an
orchestrator represents yet-another system for which platform providers must fix
bugs, maintain high availability, and provision resources.

In this paper, we try to answer a simple question: \textit{can the benefits of a
workflow  orchestrator, namely ease-of-programming, complex compositions, and
failure handling, without requiring an orchestrator-specific service at
reasonable performance and cost?}.

We propose \name{}, a \textit{decentralized} serverless workflow system that
runs in-situ with user defined FaaS functions \textit{on existing, minimal,
serverless infrastructure}. \name{} supports the same class of applications as
centralized orchestrators, has similar or better performance, and can achieve
better scalability than some widely used centralized orchestrators. Compared to
Step Functions, a representative set of applications cost up to 27.3x less, and
run up to 4.5x faster using \name{}.

Importantly, \name{}'s decentralization does not sacrifice any of the   fault
tolerance and strong execution guarantees of  commonly used orchestrators
\cite{aws-step-functions, durable-functions, google-cloud-composer,
google-workflows}.  In particular, \name{} guarantees that workflows produce
consistent results. There is no risk of workflow divergence due to
unanticipated function nondeterminism or concurrency---a common source of
errors when developers attempt to compose serverless functions without the
help of a workflow system.

%In particular, for side-effect-free functions, this guaranteeThis matches the guarantees provided by central orchestrators, such as Step Functionsexecution semantics even if an application's constituent function
%fails and is retried or is otherwise run multiple times by the FaaS system.
%Even if some constituent functions execute multiple times, the final application
%states appear as if all functions executed once.

%\dhl{Design insights, highlights and overview}

% \name{} uses a two-stage compiler that tackles the complexity of orchestration
% at compile-time and removes the need to use hosted orchestrator services for
% workflow execution. The compiler transforms workflow definitions (e.g., Step
% Functions state machines) to a continuation-based intermediary representation
% (the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
% constiuent functions such that each function is responsible for invoking its
% immediate downstream functions.

% Workflow functions in \name{} are deployed with a runtime wrapper that
% transparently interposes on user code entry and exit. \name{} runtime wrapper
% does not change how users build functions. Developer can still write functions
% exactly the same way as if they are individual functions, and do not need to
% import any additional libraries in order to use \name{}.

% During execution, when user code completes, the runtime wrapper executes the
% assigned continuations which triggers its immediate downstream functions in
% the workflow. Each function performs the same action in the order defined in
% the workflow such that the orchestration logic is distributedly executed by
% the collection of constituent functions.\shadi{this last sentence is a bit vague, reword.}

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

% We present and evaluate an implementation of \name{} that can compile Step
% Functions state machines and execute them purely as Lambda functions using
% either S3 or DynamoDB as the data store. The implementation supports all
% orchestration patterns in Step Functions. Our experimental results show that,
% compared with Step Functions, \name{} improves performance by 11-28\% in the
% case of chaining functions and up to 4.5x in high-parallelism patterns such as
% fan-out. At the same time, \name{} significantly reduces the cost of running
% applications by 3.7x to 32.8x.

This paper makes the following contributions:

\squishlist

  \item A low-level intermediate representation that partitions workflow logic
  to functions.

  \item A runtime that implements common orchestration patterns
  (e.g., chaining, branching, fan-out, fan-in) in a decentralized manner and
  run in-situ with user-defined FaaS functions and

  \item An implementation of the \name{} runtime for AWS Lambda and 
DynamoDB as well as a compiler from the AWS Step Functions language to
\name{}'s low level IR

\squishend
% -------

% execution guarantee challenges.
