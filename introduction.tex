\section{Introduction}

Serverless computing provides a simple and yet powerful abstraction for
building distributed applications. \shadi{add citations on serverless popularity. maybe even add some numbers in the text, e.g., x\% growth of adoption or how quickly it is becoming increasingly popular. } Developers build event-driven functions
without the need to provision and manage resources. Serverless platforms
execute functions with fast autoscaling that can start thousands of instances
in seconds, provide fine-grained billing that only charges for resources used, and
enable improved utilization that can reclaim resources immediately when they become
idle.

There are strong interests in building large serverless applications composed
of multiple functions with rich patterns of interactions~\cite{excamera,
pywren, gg-atc, beldi, boki}. \shadi{suggestion: maybe name a few apps to give
an intuition of the complexity. } These applications call for a
\emph{higher-level programming interface} to express function interactions and
\emph{strong end-to-end guarantees} for correct executions.

Several serverless workflow systems have emerged to meet the challenges,
offering various programming interfaces (e.g., state machines and DAG) for
defining workflows that are composed of multiple connected functions and
varying levels of execution guarantees~\cite{excamera, gg-atc,
aws-step-functions, google-cloud-composer, google-workflows,
durable-functions}. Designs of existing solutions all require adding
supplemental hosted services to current infrastructures for workflow
execution. These long-running hosted services, often called controllers,
coordinators or orchestrators, execute workflow definitions, invoke
constituent functions and manage workflow states.

% However, designs of existing solutions all
% break out of the serverless abstraction by requiring supplemental hosted
% services added to current infrastructures. These long-running hosted services,
% often called controllers, coordinators or orchestrators, execute workflow
% definitions, manage workflow states and broker inter-function communication.

However, designs that rely on long-running hosted orchestrators break out of
the serverless abstraction and compromise key benefits of serverless
computing. Besides the costs of building and maintaining a hosted service
which often requires a dedicated engineering team and end-to-end performance now
also depends on the orchestrator. A slow orchestrator can become a
bottleneck and nullify the fast autoscale advantage of serverless (as we
discovered and show in \S~\ref{sec:eval-fan-out}).

Furthermore, for every workflow invocation, an orchestrator instance needs to
stay until all functions in the workflow complete, including idle time waiting
for functions to return (Figure~\ref{fig:orchestrator-design}). This
complicates resource multiplexing and reduces utilization. In turn, service
providers pass the costs of the lost efficiency to users by employing
separately-designed pricing schemes that are not fine-grained,
pay-for-what-you-use billing and essentially ``double-charging'' for idle
orchestrator resources that they cannot immediately reclaim.

In this paper, we propose \name{}, a new serverless workflow system design
that allows developers to write workflows in existing higher-level programming
interfaces and executes them purely as event-driven functions.

\name{} uses a two-stage compiler that tackles the complexity of orchestration
at compile-time and removes the need to use hosted orchestrator services for
workflow execution. The compiler transforms workflow definitions (e.g., Step
Functions state machines) to a continuation-based intermediary representation
(the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
constiuent functions such that each function is responsible for invoking its
immediate downstream functions.

Workflow functions in \name{} are deployed with a runtime wrapper that
transparently interposes on user code entry and exit. \name{} runtime wrapper
does not change how users build functions. Developer can still write functions
exactly the same way as if they are individual functions, and do not need to
import any additional libraries in order to use \name{}.

During execution, when user code completes, the runtime wrapper executes the
assigned continuations which triggers its immediate downstream functions in
the workflow. Each function performs the same action in the order defined in
the workflow such that the orchestration logic is distributedly executed by
the collection of constituent functions.\shadi{this last sentence is a bit vague, reword.}

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

\dhl{State the execution guarantees without details on how.}

We present and evaluate an implementation of \name{} that can compile Step
Functions state machines and execute them purely as Lambda functions using
either S3 or DynamoDB as the data store. The implementation supports all
orchestration patterns in Step Functions (i.e., chaining, branching,
\texttt{Map} and \texttt{Parallel} fan-out and fan-in) and can additionally
express fold and pipeline parallelism.

We evaluate the performance and cost benefits of \name{} with a suite of
microbenchmarks and 4 real-world applications. Our experimental results show
that, compared with Step Functions, \name{} improves performance by 11-28\% in
the case of chaining functions and up to 4.5x in high-parallelism patterns
such as fan-out. At the same time, \name{} significantly reduces the cost of
running applications by 3.7x to 32.8x.

This paper makes the following contributions:

\begin{itemize}

  \item A set of general algorithms, called gadgets, that implement all common
  orchestration patterns (e.g., chaining, branching, fan-out, fan-in) with
  serverless functions and serverless storage.

  \item An intermediate representation language that can express complex
  workloads using these gadgets in a platform agnostic way, and a front-end
  compiler that transforms Step Functions state machines to the IR.

  \item An implementation of the \name{} runtime that execute workflows purely
  as Lambda functions using either S3 or DynamoDB as the data store for enable
  fan-in and provide exactly-once semantics.

\end{itemize}
% -------

% execution guarantee challenges.
