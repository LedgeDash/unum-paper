\section{Introduction}

Serverless computing provides a simple and yet powerful abstraction that
consists of a stateless compute component (FaaS) and autoscaling,
pay-for-what-you-use data stores (BaaS)~\cite{berkeley}. Developers build
applications in the form of event-driven functions without the need to
provision and manage resources. Serverless platforms execute functions with
fast autoscaling that can start thousands of instances in seconds, provide
fine-grained billing that only charges for resources used, and enable improved
utilization that can reclaim resources immediately when they become idle.

There are strong interests in building large serverless applications composed
of multiple functions with rich patterns of interactions~\cite{excamera,
pywren, gg-atc, beldi, boki}. In addition to building and executing individual
functions, these workloads call for a \emph{higher-level programming
interface} that expresses interactions between functions and \emph{strong
end-to-end guarantees} for correct executions.

Several serverless workflow systems have emerged to meet the demand, offering
various specialized programming interfaces (e.g., state machines and DAG) for
expressing function interactions and varying levels of execution
guarantees~\cite{excamera, gg-atc, aws-step-functions, google-cloud-composer,
google-workflows, durable-functions}. Designs of existing solutions all
require adding an additional stateful component, often called an orchestrator
or controller, to the current serverless infrastructure. An orchestrator is a
hosted service separate from FaaS and BaaS. For each workflow invocation, an
orchestrator instance interpret the workflow definition and drives workflow
execution by invoking constituent functions, waits for their results and pass
them to downstream functions. All function invocations are initiated by the
orchestrator and all workflow states (e.g., function results) flow through the
orchestrator.

However, designs that rely on long-running hosted orchestrators break out of
the serverless abstraction and compromise key benefits of serverless
computing. Besides the costs of building and maintaining a hosted service
which often requires a dedicated engineering team and end-to-end performance
now also depends on the orchestrator. A slow orchestrator can become a
bottleneck and nullify the fast autoscale advantage of serverless (as we
discover and show in \S~\ref{sec:eval-fan-out}).

Furthermore, for every workflow invocation, an orchestrator instance needs to
stay until all functions in the workflow complete, including idle time waiting
for functions to return (Figure~\ref{fig:orchestrator-design}). This
complicates resource multiplexing and reduces utilization. In turn, service
providers pass the costs of the lost efficiency to users by employing
separately-designed pricing schemes that are not fine-grained,
pay-for-what-you-use billing and essentially ``double-charging'' for idle
orchestrator resources that they cannot immediately reclaim.

In this paper, we propose \name{}, a new serverless workflow system design
that allows developers to write workflows in existing higher-level programming
interfaces and executes them purely as event-driven functions.

\name{} uses a two-stage compiler that tackles the complexity of orchestration
at compile-time and removes the need to use hosted orchestrator services for
workflow execution. The compiler transforms workflow definitions (e.g., Step
Functions state machines) to a continuation-based intermediary representation
(the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
constiuent functions such that each function is responsible for invoking its
immediate downstream functions.

Workflow functions in \name{} are deployed with a runtime wrapper that
transparently interposes on user code entry and exit. \name{} runtime wrapper
does not change how users build functions. Developer can still write functions
exactly the same way as if they are individual functions, and do not need to
import any additional libraries in order to use \name{}.

During execution, when user code completes, the runtime wrapper executes the
assigned continuations which triggers its immediate downstream functions in
the workflow. Each function performs the same action in the order defined in
the workflow such that the orchestration logic is distributedly executed by
the collection of constituent functions.\shadi{this last sentence is a bit vague, reword.}

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

\name{} provides a set of execution guarantees that serverless workflows care
about. Specifically, \name{} ensures at-least-once execution on constituent
function, which guarantees that workflow execution does not hang
mid-execution. If a workflow execution crashes, \name{} automatically retries
the function where the crash happens without restarting the worklfow from the
beginning. Furthermore, \name{} provides exactly-once semantics such that even
if a function executes multiple times, the final states appear as if it
executes once.

We present and evaluate an implementation of \name{} that can compile Step
Functions state machines and execute them purely as Lambda functions using
either S3 or DynamoDB as the data store. The implementation supports all
orchestration patterns in Step Functions (i.e., chaining, branching,
\texttt{Map} and \texttt{Parallel} fan-out and fan-in) and can additionally
express fold and pipeline parallelism. Our experimental results show that,
compared with Step Functions, \name{} improves performance by 11-28\% in the
case of chaining functions and up to 4.5x in high-parallelism patterns such as
fan-out. At the same time, \name{} significantly reduces the cost of running
applications by 3.7x to 32.8x.

This paper makes the following contributions:

\begin{itemize}

  \item A set of general algorithms, called gadgets, that implement all common
  orchestration patterns (e.g., chaining, branching, fan-out, fan-in) with
  serverless functions and serverless storage.

  \item An intermediate representation language that can express complex
  workloads using these gadgets in a platform agnostic way, and a front-end
  compiler that transforms Step Functions state machines to the IR.

  \item An implementation of the \name{} runtime that execute workflows purely
  as Lambda functions using either S3 or DynamoDB as the data store for enable
  fan-in and provide exactly-once semantics.

\end{itemize}
% -------

% execution guarantee challenges.
