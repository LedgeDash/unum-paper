\section{Introduction}\label{sec:intro}

Serverless computing offers a simple but powerful abstraction consisting of
stateless compute components (Functions as a Service, or FaaS) and scalable,
multi-tenant data stores~\cite{berkeley}. Developers build applications using
task-specific, event-driven ``functions'' without the need to provision or
manage servers.
Serverless computing builds on ubiquitous existing cloud
infrastructure---scalable and performant storage and workload scheduling---so is
able to expose highly granular access to datacenter resources with relatively
low overhead and end-user cost.
%
As a result, serverless platforms have been able to offer high burst-scalability
and quickly reclaim function resources as soon as they are
idle~\cite{aws-lambda,gcp-functions,azure-functions,openwhisk,openfaas}.
Developers are charged in small compute and storage increments (100s of
milliseconds and per-stored-object and request, respectively), and platforms can
make efficient use of hardware resources.

While serverless platforms originally targeted simple applications with one or a
few functions, this paradigm has increasingly proven useful for more complex
applications composed of many functions with rich, stateful, interaction
patterns~\cite{excamera, pywren, gg-atc, beldi, boki}. Unfortunately, building
such applications using basic serverless building blocks is challenging.
At-least once execution guarantees mean functions with a non-deterministic output may
pass inconsistent results to downstream functions. Event-driven execution makes
depending on the results of multiple previous functions challenging. Finally,
intermediate states must be managed efficiently and deleted promptly to avoid
incurring unbounded storage costs over repeated executions of a workflow.

Cloud providers have deployed specialized centralized workflow
orchestrators~\cite{aws-step-functions, google-cloud-composer, google-workflows,
durable-functions} to support such applications. Developers provide a
description of an execution graph---nodes in the graph represent FaaS functions
and edges represent invocations of a function with the output of one or more
functions---while the orchestrator drives the execution of this graph and hosts
the state of execution (outstanding invocations and intermediate results)
centrally.

Centralization makes supporting complex interactions simple---e.g.\ an
orchestrator can support fan-in patterns by simply waiting for all branches to
complete before invoking an aggregation function. Similarly, a centralized
orchestrator can ensure that workflow results appear to be the result of
executing each constituent function exactly-once by choosing one result for
each function invocation. Finally, centralizing orchestration into a
multi-tenant service amortizes the engineering and resource cost of making such
a service fault tolerant, correct, and scalable. As a result, these
orchestrators can offer high-level programming interfaces that express complex
function interactions while promising that workflow results are consistent even
if individual function invocations execute more than once.

%In principle, developers could deploy their own orchestrators, written as
%sequential programs running on a dedicated machine function. However, such a
%solution isn't tolerant to faults in the orchestrator and does not provide burst
%scalability to many workflow invocations. Instead, application developers
%typically rely on a multi-tenant orchestration service run by the cloud provider
%that amortizes the engineering and resource cost of making orchestration fault
%tolerant, correct, and scalable. In turn, these overheads are typically
%reflected in high end-user prices compared to invoking FaaS functions natively.

However, centralized orchestrators also preclude the user from making their own
trade-offs between available interactions or execution guarantees and
performance, resource overhead, scalability and expressiveness. For example, an
application with some deterministic functions in their workflow may not be able
to reap performance benefits for those functions. This results in a compromise
familiar from operating systems~\cite{exokernel,spin},
networks~\cite{active-networks,sdn}, and storage
systems~\cite{comet,splinter}---well designed centralized systems efficiently
support the needs of many applications but cannot meet all application needs.
%
%Indeed, researchers have devised new orchestrators to support FaaS applications
%that cannot be efficiently run using existing
%orchestrators~\cite{excamera,gg-atc}.

In this paper, we try to answer a simple question: \textit{is centralization
\emph{necessary} to achieve the benefits of workflow  orchestrators?} We argue
it is not. In particular, scalable and strongly consistent datastores, which already serve as a critical component of serverless platforms, resolve the hardest
challenge of centralization: coordination. Using such datastores, we show that a
decentralized orchestrator can achieve strong execution guarantees and run
complex execution graphs efficiently. We argue that decentralizing orchestration
is better for cloud providers as they need not develop and maintain yet another
complex service. It is better for developers as it gives applications more
flexibility to use more performant, applications-specific orchestration
optimizations and makes porting applications between different cloud platforms
easier.

To support these arguments, we present \name{}, a system for decentralized
serverless workflow orchestration (\S\ref{sec:design}). \name{} provides
orchestration as a library that runs in-situ with user defined FaaS functions,
rather than as a separate service. The library relies on a minimal set of
existing serverless APIs---function invocation and a few basic datastore
operations---that are common across cloud platforms. \name{} introduces an
intermediate representation (IR) language to express interactions execution
graphs using only node-local information while supporting front-end compilers
that can transform high-level workflow descriptions into the IR.

At a high level, \name{} relies on the FaaS scheduler to run each function
invocation \emph{at least} once and consistent datastore operations to
coordinate interactions and deduplicate multiple executions of the same function
invocation. The key design challenge in \name{} is to provide these properties
in a decentralized yet efficient manner using a minimal set of underlying
serverless APIs.

\name{} fan-ins use objects in a consistent datastore as a coordination point
for fan-in branches. \name{} ensures workflow correctness despite multiple
executions of non-deterministic functions by using checkpoints to commit to
exactly one output for a function invocation. Both require generating globally
unique names for nodes and edges in the execution graph \emph{locally} (using
information available at each node locally) as well as cleaning up intermediate
datastore objects in a timely manner.

Our implementation of \name{} (\S\ref{sec:impl}) includes a compiler for AWS Step
Functions' description language, enabling \name{} to run arbitrary Step Function
workflows.  We show that Step Function workflows compiled to \name{} execute
with the same execution semantics and fault-tolerance as running natively using
the Step Functions centralized orchestrator.

Moreover, while both performance and cost are difficult to compare objectively
with existing black-box production orchestrators---both are influenced by
deployment and pricing decisions that may not reflect the underlying efficiency
or cost of the system---\name{} performs well in practice (\S\ref{sec:eval}). We
find that a representative set of applications scale better with \name{} than
AWS Step Functions, typically complete faster low scale, and cost
significantly less (by over an order of magnitude) to run. We also demonstrate
that \name{}'s IR allows hand-tuned applications to run faster than their Step
Functions equivalent by using application-specific optimizations and supporting
a richer set of interaction patterns.
