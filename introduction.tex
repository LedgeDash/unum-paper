\section{Introduction}

Serverless computing provides a simple and yet powerful abstraction for
building distributed applications. Developers build event-driven functions
without the need to provision and manage resources. Serverless platforms
execute functions with fast autoscaling that can start thousands of instances
in seconds, fine-grained billing that only charges for resources used, and
improved utilization that can reclaim resources immediately when they become
idle.

There are strong interests in building large serverless applications composed
of multiple functions with rich patterns of interactions~\cite{excamera,
pywren, gg-atc, beldi, boki}. These applications call for a \emph{higher-level
programming interface} to express function interactions and \emph{strong
end-to-end guarantees} for correct executions.

Several serverless workflow systems have emerged to meet the challenges,
offering various programming interfaces for workflow definition (e.g., state
machines and DAG) and varying levels of execution guarantees~\cite{excamera,
gg-atc, aws-step-functions, google-cloud-composer, google-workflows,
durable-functions, kappa}. Designs of existing solutions all require adding
supplemental hosted services to current infrastructures for workflow
execution. These long-running hosted services, often called controllers,
coordinators or orchestrators, execute workflow definitions, invoke
constituent functions and manage workflow states.

% However, designs of existing solutions all
% break out of the serverless abstraction by requiring supplemental hosted
% services added to current infrastructures. These long-running hosted services,
% often called controllers, coordinators or orchestrators, execute workflow
% definitions, manage workflow states and broker inter-function communication.

However, designs that rely on long-running hosted orchestrators break out of
the serverless abstraction and compromise key benefits of serverless
computing. Besides the costs of building and maintaining a hosted service
which often requires a dedicated engineering team, end-to-end performance now
also depends on the orchestrator. A slow orchestrator can become a
bottleneck and nullify the fast autoscale advantage of serverless (as we
discovered and show in \S~\ref{sec:eval-fan-out}).

Furthermore, for every workflow invocation, an orchestrator instance needs to
stay until all functions in the workflow complete, including idle time waiting
for functions to return (Figure~\ref{fig:orchestrator-design}). This
complicates resource multiplexing and reduces utilization. In turn, service
providers pass the costs of the lost efficiency to users by employing
separately-designed pricing schemes that are not fine-grained,
pay-for-what-you-use billing and essentially ``double-charging'' for idle
orchestrator resources that they cannot immediately reclaim.

In this paper, we propose \name{}, a new serverless workflow system design
that allows developers to write workflows in existing higher-level programming
interfaces and executes them purely as event-driven functions.

\name{} uses a two-stage compiler that tackles the complexity of orchestration
at compile-time and removes the need to use hosted orchestrator services for
workflow execution. The compiler transforms workflow definitions (e.g., Step
Functions state machines) to a continuation-based intermediary representation
(the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
constiuent functions such that each function is responsible for invoking its
immediate downstream functions.

Workflow functions in \name{} are deployed with a runtime wrapper that
transparently interposes on user code entry and exit. \name{} runtime wrapper
does not change how users build functions. Developer can still write functions
exactly the same way as if they are individual functions, and do not need to
import any additional libraries in order to use \name{}.

During execution, when user code completes, the runtime wrapper executes the
assigned continuations which triggers its immediate downstream functions in
the workflow. Each function performs the same action in the order defined in
the workflow such that the orchestration logic is distributedly executed by
the collection of constituent functions.

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

Similar to some prior work~\cite{boki, beldi}, \name{} leverages strongly
consistent data stores to provide strong execution guarantees. \name{}'s
guarantees are on a per-function-invocation granularity. Each function
invocation creates a uniquely named checkpoint when user code completes. The
checkpoint ensures that re-executions, due to failure-related retries or
duplicate event delivery, never run user code again.


We present and evaluate an implementation of \name{} that can compile Step
Functions state machines and execute them purely as Lambda functions using
either S3 or DynamoDB as the data store. The implementation supports all
orchestration patterns in Step Functions (i.e., chaining, branching,
\texttt{Map} and \texttt{Parallel} fan-out and fan-in) and can additionally
express fold and pipeline parallelism.

We evaluate the performance and cost benefits of \name{} with a suite of
microbenchmarks and 4 real-world applications. Our experimental results show
that \name{} is slightly faster (11-28\%) than Step Functions in chaining
performance, and much faster (up to 4.58x) in fan-out, especially at high
levels of parallelism. Step Functions seems to suffer from throttling at even
moderate levels with only 20 parallel branches.

Furthermore, even when using the faster but more expensive DynamoDB as the
data store, for all applications evaluated, running on \name{} is more than
3.6x cheaper than Step Functions, and for all but one application, running on
\name{} is more than one order-of-magnitude cheaper than Step Functions.

Last but not least, \name{}'s key design choice to build upon the existing
serverless abstraction means that the performance of \name{} can automatically
improve when the underlying services improve. Our results show that over 97\%
of \name{}'s latency overhead comes from API calls to Lambda and data stores,
which means the bulk of \name{}'s overhead could quickly diminish with a
faster Lambda or data store, without any modification to \name{} itself.

% -------

% execution guarantee challenges.
