\section{Introduction}

Serverless computing offers a simple but powerful abstraction consisting of
stateless compute component (Functions as a Service, or FaaS) and scalable,
multi-tenant data stores~\cite{berkeley}. Developers build applications using
task-specific, event-driven ``functions'' without the need to provision or
manage servers. In turn, serverless platforms have been able to offer high
burst-scalability and quickly reclaim function resources as soon as they are
idle. Developers are charged in small compute and storage increments (100s of
milliseconds and per-stored-object and request, respectively), and platforms
can make efficient use of hardware resources.

Originally, serverless platforms targeted simple applications with one or a few
functions and simple interaction patterns. Recently, though, there has been
emerging interest in using serverless platforms to build more complex
applications composed of 10s to 100s functions with rich interaction
patterns~\cite{excamera, pywren, gg-atc, beldi, boki}. Unfortunately, building
such applications using basic serverless building blocks is challenging.
Handling complex control flows and large numbers of functions, in a ad-hoc
manner is difficult to both express and implement correctly.  Additionally, FaaS
invocation semantics requires application developers to gracefully handle
individual function failures and retries.

To meet this need, several workflow systems have emerged to enable serverless
applications with large and complex ``workflows'' of functions. These offer
high-level programming interfaces to express function interactions, supporting a
rich set of composition patterns for chaining, branching, fan-out and fan-in, as
well as exactly-once workflow execution semantics~\cite{excamera, gg-atc,
aws-step-functions, google-cloud-composer, google-workflows, durable-functions}.
These systems rely on some form of \emph{orchestrator} to track workflow
progress, implement interaction logic, and correctly handle retries of
individual steps due to faults or inconsistency in function invocation queues.

An orchestrator is a stateful component that centralizes execution states
and control-flow operations. For each workflow transition in a serverless
application, the orchestrator waits for currently running functions to complete,
determines what to do with their results, and which function(s) to run
next. All function invocations are initiated by the orchestrator, and all
workflow states (e.g., function results) flow through the orchestrator. Using a 
centralized orchestrator simplifies failure handling, as the entire workflow state resides
in the orchestrator and can be easily checkpointed. However, it is also rather
communication-intensive, as all control and data must flow back and forth between
the orchestrator and the functions.

%Such centralized orchestrators enable new classes of serverless applications,
%but requires an additional multi-tenant service to be maintained and operated
%and adds additional contraints on performance and scalability.  
Another downside of many central orchestrators is that they are not serverless themselves.
Commonly, the orchestrator functionality is provided by a separate stateful hosted service
\cite{aws-step-functions, google-cloud-composer, google-workflows}. This requires
development and operation of yet another scalable, multi-tenant, fault-tolerant, 
billable service, in addition to the already existing serverless compute and storage 
infrastructure. 

While basic
serverless build blocks are the result of decades of research and engineering to
achieve performant and scalable compute and storage infrastructure,
orchestrators perform specialized functionality that must solve
scalability and performance problems independently. Applications may suffer, for
example, from scalability bottlenecks in the orchestrator even if storage and
FaaS platforms can support much higher burst-scalability. Moreover, an
orchestrator represents yet-another system for which platform providers must fix
bugs, maintain high availability, and provision resources.

In this paper, \shadiS{we ask a question: \textit{can we achieve the benefits of a workflow  orchestrator, namely ease-of-programming, complex compositions, and failure handling, without having an additional orchestrator service?}}.

We propose \name{}, a \textit{decentralized} serverless workflow system
that runs in-situ with user defined FaaS functions \textit{on existing, minimal,
serverless infrastructure}. \name{} supports the same class of applications as
centralized orchestraters, has similar or better performance, and can achieve
better scalability than some widely used centralized orchestraters. Compared to
Step Functions, a representative set of applications cost up to 32.8x less, and
run up to 4.5x faster using \name{}.

%Importantly, \name{}'s decentralization does not sacrifice properties important
%to complex serverless applications. It can provide the same set of execution
%guarantees as centralized orchestrators. Specifically, \name{} guarantees
%exactly-once execution semantics: even if an application's constituent function
%fails,  is retried, or is otherwise ran multiple times, the final application
%states appear as if all functions executed once.

Importantly, \name{}'s decentralization does not sacrifice any of the  
fault tolerance and strong execution guarantees of 
commonly used orchestrators \cite{aws-step-functions, durable-functions,
 google-cloud-composer, google-workflows}. 
Just like the latter, 
\name{} guarantees that workflows do not get stuck (functions are retried if there are faults)
and produce consistent results (for each constituent function, exactly one result is recorded).
In particular, there is no risk of workflow divergence due to unanticipated function
nondeterminism or concurrency, which is a common source of errors
when developers attempt to compose serverless functions without the help
of a workflow system.

%In particular, for side-effect-free functions, this guaranteeThis matches the guarantees provided by central orchestrators, such as Step Functionsexecution semantics even if an application's constituent function
%fails and is retried or is otherwise run multiple times by the FaaS system.
%Even if some constituent functions execute multiple times, the final application
%states appear as if all functions executed once.

%\dhl{Design insights, highlights and overview}

% \name{} uses a two-stage compiler that tackles the complexity of orchestration
% at compile-time and removes the need to use hosted orchestrator services for
% workflow execution. The compiler transforms workflow definitions (e.g., Step
% Functions state machines) to a continuation-based intermediary representation
% (the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
% constiuent functions such that each function is responsible for invoking its
% immediate downstream functions.

% Workflow functions in \name{} are deployed with a runtime wrapper that
% transparently interposes on user code entry and exit. \name{} runtime wrapper
% does not change how users build functions. Developer can still write functions
% exactly the same way as if they are individual functions, and do not need to
% import any additional libraries in order to use \name{}.

% During execution, when user code completes, the runtime wrapper executes the
% assigned continuations which triggers its immediate downstream functions in
% the workflow. Each function performs the same action in the order defined in
% the workflow such that the orchestration logic is distributedly executed by
% the collection of constituent functions.\shadi{this last sentence is a bit vague, reword.}

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

% We present and evaluate an implementation of \name{} that can compile Step
% Functions state machines and execute them purely as Lambda functions using
% either S3 or DynamoDB as the data store. The implementation supports all
% orchestration patterns in Step Functions. Our experimental results show that,
% compared with Step Functions, \name{} improves performance by 11-28\% in the
% case of chaining functions and up to 4.5x in high-parallelism patterns such as
% fan-out. At the same time, \name{} significantly reduces the cost of running
% applications by 3.7x to 32.8x.

This paper makes the following contributions:

\squishlist

  \item An end-to-end system that can take workflows written in high-level
  language and execute them purely as FaaS functions.

  \item A \shadiS{decentralized design for}
  %set of general algorithms, called gadgets, that implement
   all common
  orchestration patterns (e.g., chaining, branching, fan-out, fan-in) using \shadiS{only}
  serverless functions and serverless storage.

  \item An intermediate representation language that can express complex
  workloads in a platform agnostic way, and a front-end
  compiler that transforms Step Functions state machines to the IR.

  \item An implementation of the \name{} runtime that executes workflows purely
  as Lambda functions and uses either S3 or DynamoDB as the data store in order to enable
  fan-in and provide exactly-once semantics. \shadi{this last bullet point is out of place. the first one already mentions system on FaaS. is it trying to highlight the exactly-once? the datastore? or the fact that we have an implementation? }

\squishend
% -------

% execution guarantee challenges.
