\section{Introduction}

Serverless computing offers a simple but powerful abstraction consisting of
stateless compute component (Functions as a Service, or FaaS) and scalable,
multi-tenant data stores~\cite{berkeley}. Developers build applications using
task-specific, event-driven ``functions'' without the need to provision or
manage servers. In turn, serverless platforms have been able to offer high
burst-scalability and quickly reclaim function resources as soon as they are
idle. Developers are charged in small compute and storage increments (100s of
milliseconds and per-stored-object and request, respectively), and platforms
can make efficient use of hardware resources.

Originally, serverless platforms targeted simple applications with one or a few
functions and simple interaction patters. Recently, though, there has been
emerging interest in using serverless platforms to build complex applications
composed of many functions with rich interaction patterns~\cite{excamera,
pywren, gg-atc, beldi, boki}. Unfortunately, building such applications using
basic serverless building blocks is challenging. FaaS invocation semantics
require applications to handle individual function failures and retries
gracefully, and driving complex control flows in a decentralized manner is both
difficult to implement correctly and difficult to express.

To meet this need, several fault-tolerant workflow systems have emerged. These offer higher-level
programming interfaces to express function interactions, suppporting a rich set of
composition patterns that support chaining, branching, fan-out and fan-in, and
exactly-once semantics for workflow execution~\cite{excamera, gg-atc,
aws-step-functions, google-cloud-composer, google-workflows, durable-functions}.
All of these systems rely on some form of \emph{orchestrator} to track workflow progress
and retry individual steps in the caase of faults.

An orchestrator is a stateful component that centralizes execution states
and control-flow operations. For each workflow transition in a serverless
application, the orchestrator waits for currently running functions to complete,
determines what to do with their results, and which function or functions to run
next. All function invocations are initiated by the orchestrator, and all
workflow states (e.g., function results) flow through the orchestrator. Using a 
centralized orchestrator simplifies failure handling, as the entire workflow state resides
in the orchestrator, where it can be easily checkpointed. However, it is also rather
communication-intensive, as all control and data must flow back and forth between
the orchestrator and the functions.

%Such centralized orchestrators enable new classes of serverless applications,
%but requires an additional multi-tenant service to be maintained and operated
%and adds additional contraints on performance and scalability.  
Another downside of many central orchestrators is that they are not serverless themselves.
Commonly, the orchestrator functionality is provided by a separate stateful hosted service
\cite{aws-step-functions, google-cloud-composer, google-workflows}. This requires
development and operation of yet another scalable, multi-tenant, fault-tolerant, 
billable service, in addition to the already existing serverless compute and storage 
infrastructure. 

While basic
serverless build blocks are the result of decades of research and engineering to
achieve performant and scalable compute and storage infrastructure,
orchestrators provide perform specialized functionality that must solve
scalability and performance problems independently. Applications may suffer, for
example, from scalability bottlenecks in the orchestrator even if storage and
FaaS platforms can support much higher burst-scalability. Moreover, an
orchestrator represents yet-another system for which platform providers must fix
bugs, maintain high availability, and provision resources.

In this paper, we propose \name{}, a decentralized serverless workflow system
that runs in-situ with user defined FaaS functions on existing, minimal,
serverless infrastructure. \name{} supports the same class of applications as
centralized orchestrators, has similar or better performance, and can achieve
better scalability than some widely used centralized orchestrators. Compared to
Step Functions, a representative set of applications cost up to 32.8x less, and
run up to 4.5x faster using \name{}.

Importantly, \name{}'s decentralization does not sacrifice properties important
to complex serverless applications. It can provide the same set of execution
guarantees as centralized orchestrators. Specifically, \name{} guarantees
exactly-once execution semantics even if an application's constituent function
fails and is retried or is otherwise run multiple times by the FaaS system.
Even if some constituent functions execute multiple times, the final application
states appear as if all functions executed once.

%\dhl{Design insights, highlights and overview}

% \name{} uses a two-stage compiler that tackles the complexity of orchestration
% at compile-time and removes the need to use hosted orchestrator services for
% workflow execution. The compiler transforms workflow definitions (e.g., Step
% Functions state machines) to a continuation-based intermediary representation
% (the \name{} IR \S\ref{sec:design-ir}) and distributes the continuations to
% constiuent functions such that each function is responsible for invoking its
% immediate downstream functions.

% Workflow functions in \name{} are deployed with a runtime wrapper that
% transparently interposes on user code entry and exit. \name{} runtime wrapper
% does not change how users build functions. Developer can still write functions
% exactly the same way as if they are individual functions, and do not need to
% import any additional libraries in order to use \name{}.

% During execution, when user code completes, the runtime wrapper executes the
% assigned continuations which triggers its immediate downstream functions in
% the workflow. Each function performs the same action in the order defined in
% the workflow such that the orchestration logic is distributedly executed by
% the collection of constituent functions.\shadi{this last sentence is a bit vague, reword.}

%  a runtime wrapper transparently interposes on user code
% entry and exit and executes the assigned continuations when user code
% completes. The \name{} runtime wrapper does not change how users build
% functions. Developer can still write functions exactly the same way as if they
% are individual functions, and do not need to import any additional libraries
% in order to use \name{}.

% We present and evaluate an implementation of \name{} that can compile Step
% Functions state machines and execute them purely as Lambda functions using
% either S3 or DynamoDB as the data store. The implementation supports all
% orchestration patterns in Step Functions. Our experimental results show that,
% compared with Step Functions, \name{} improves performance by 11-28\% in the
% case of chaining functions and up to 4.5x in high-parallelism patterns such as
% fan-out. At the same time, \name{} significantly reduces the cost of running
% applications by 3.7x to 32.8x.

This paper makes the following contributions:

\squishlist

  \item An end-to-end system that can take workflows written in higher-level
  language and execute them purely as FaaS functions.

  \item A set of general algorithms, called gadgets, that implement all common
  orchestration patterns (e.g., chaining, branching, fan-out, fan-in) with
  serverless functions and serverless storage.

  \item An intermediate representation language that can express complex
  workloads using these gadgets in a platform agnostic way, and a front-end
  compiler that transforms Step Functions state machines to the IR.

  \item An implementation of the \name{} runtime that execute workflows purely
  as Lambda functions using either S3 or DynamoDB as the data store for enable
  fan-in and provide exactly-once semantics.

\squishend
% -------

% execution guarantee challenges.
