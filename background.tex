\section{Background \& Motivation}\label{sec:bg}

The basic serverless abstraction is simple and quite powerful. Developers
build ``functions'', typically written in a high-level language and packaged
as OS containers or virtual machines, which run short computations in response
to a platform event. Events include storage events (e.g., the creation of an
object) or HTTP requests. The platform can scale resources for each function
to respond to instantaneous bursts in events and developers are absolved from
capacity planning and resource management tasks.

% For example, a developer builds a function that creates thumbnails from
% large images and configures the platform to trigger the function whenever a
% new image is uploaded to their object store. When there is a burst of such
% events---e.g\ if an end-user uploads their whole photo library at once---the
% platform can scale by spawning many parallel instances of the function to
% respond to events within seconds and reclaim resources when the bust
% subsides.

This simple abstraction can be used to compose many simple applications with
one or a few functions. For example, developers can chain functions for data
pipelines using triggers. In trigger-based composition~\cite{netherite} each
function in a chain invokes the next asynchronously or writes to a data store
configured to invoke the next function in response to storage events.
Alternatively, developers might use a ``driver-function''~\cite{beldi} to
drive more intricate control-flow logic. A driver function acts as a
centralized controller that invokes other functions, waits for their outputs,
and invokes subsequent functions with their outputs.

Such ad-hoc approaches work ``out-of-the-box'', that is, they require no
additional platform provided infrastructure. However neither is well suited to
complex applications with 10s or 100s of functions~\cite{excamera,
hello-retail}. Trigger-based composition can only support chaining of
individual functions or fan-out from one function to multiple, but cannot, for
example, fan-in from multiple functions to one. Moreover, trigger-based
composition scatters control-flow logic across each function or in configured
storage events, making development unwieldy when application complexity grows.

On the other hand, driver functions concentrate control flow in a single
function and support arbitrary composition. However, most serverless platforms
impose modest runtime limits on individual functions, and thus driver
functions restrict the total runtime of applications. Furthermore, driver
functions suffer ``double billing'' since they are billed for the entire
call-graph execution despite spending most time idly waiting for callees to
return.

Finally, both ad-hoc approaches require developers to handle function crashes,
retries and duplicate invocations gracefully~\cite{aws-lambda-retry,
azure-functions-retry, aws-lambda-async-invoke,
azure-functions-exec-guarantee}. Application typically want to ensure
``exactly once'' semantics~\cite{netherite, beldi, boki,
formal-foundation-exec-gtnee, durable-semantics} for an entire call-graph, but
failures and multiple invocations of individual functions can subvert this
goal without careful consideration.

\subsection{Standalone Orchestrators}\label{sec:bg:orchestrator}

A common solution to address the needs of complex serverless
applications is to introduce a workflow orchestrator that provides a
high-level programming interface with support for a rich set of patterns
(e.g., branching, chaining, fan-out and fan-in)~\cite{excamera, gg-atc,
aws-step-functions, google-cloud-composer, google-workflows,
durable-functions,temporal}. Many cloud providers offer serverless
orchestrators as a service~\cite{aws-step-functions, google-cloud-composer,
google-workflows, durable-functions} or users can build custom
orchestrators~\cite{temporal, gg-atc, excamera} and deploy them in VMs
alongside their functions.

Similar to driver functions, orchestrators operate as \emph{logically
centralized} controllers. They drive a workflow by invoking its functions and
hosting application states such as function outputs and outstanding
invocations.

However, different from driver-functions, orchestrators are standalone
services. Orchestrators are not limited by function timeouts and can be
arbitrarily long-running~\cite{aws-step-functions-quotas}. Moreover, as
standalone services, orchestrators are often internally distributed and employ
techniques such as replication and sharding to provide strong execution
guarantees, fault-tolerance and scalability. For example, orchestrators can
ensure that workflows appear to execute exactly-once by choosing one result
for each function invocation, even if FaaS engines only guarantee at-least
once execution. Orchestrators can also persist or replicate states during
execution so that in face of orchestrator failures, applications do not lose
executions or retry from the beginning.

While orchestrators are able to address the needs of complex serverless
applications, introducing a new standalone service has significant drawbacks.
Building performant, scalable and fault-tolerant multi-tenant systems is hard
and orchestrators introduce yet-another potential performance and scalability
bottleneck. Indeed, we find that, in practice, production systems limit
end-to-end performance for highly-parallel applications (\S~\ref{sec:eval}).

Moreover, hosting such services is expensive. Deploying a custom orchestrator
per user risks under-utilization as it cannot multiplex over many users and
users pay even when the orchestrator is not actively in use, breaking the
fine-grained billing benefit of serverless. Provider-hosted orchestrators are
multi-tenant and can amortize this cost. But they still incur engineering
expenses as they require teams on-call. Indeed, we find that provider-hosted
orchestrators cost developers significantly and dominate the total cost of
running applications(\S~\ref{sec:eval}).

Lastly, provider-hosted orchestrators preclude users from making
application-specific optimizations. Each provider typically offers just a
single orchestrator service option. While the interface and implementation of
the orchestrator might efficiently support many applications, it cannot meet
all applications' needs, resulting in a compromise familiar from operating
systems~\cite{exokernel,spin}, networks~\cite{active-networks,sdn}, and
storage systems~\cite{comet,splinter}. Indeed, we find that provider-hosted
orchestrators force applications to compromise performance by using
less-efficient patterns (\S~\ref{sec:eval}).