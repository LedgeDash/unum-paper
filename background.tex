\section{Background \& Motivation}\label{sec:bg}

The basic serverless abstraction is simple and quite powerful. Developers
build ``functions'', typically written in a high-level language and packaged
as OS containers or virtual machines, which run short computations in response
to a platform event. Events include storage events (e.g.\ the creation of an
object) or HTTP requests. The platform can scale resources for each function
to respond to instantaneous bursts in events and developers are absolved from
capacity planning and resource management tasks.

% For example, a developer builds a function that creates thumbnails from
% large images and configures the platform to trigger the function whenever a
% new image is uploaded to their object store. When there is a burst of such
% events---e.g\ if an end-user uploads their whole photo library at once---the
% platform can scale by spawning many parallel instances of the function to
% respond to events within seconds and reclaim resources when the bust
% subsides.

This simple abstraction can be used to compose many simple applications with
one or a few functions. For example, developers can chain functions for data
pipelines using triggers. In trigger-based composition~\cite{netherite} each
function in a chain invokes the next asynchronously or each function writes to
a data-store configured to invoked the next function in response to the
storage event. Alternatively, developers might use a
``driver-function''~\cite{beldi} to drive more intricate control-flow logic. A
driver function acts as a centralized controller that invokes other functions,
waits for their responses, and invokes subsequent functions with their
responses.

Such ad-hoc approaches work ``out-of-the-box'', i.e.\ they require no
additional platform provided infrastructure. However neither is well suited to
complex applications with 10s or 100s of functions~\cite{excamera,
hello-retail}. Trigger-based composition can only support chaining of
individual functions or fan-out from one function to multiple, but cannot, for
example, fan-in from multiple functions to one. Moreover, trigger-based
composition scatters control-flow logic across each function or in configured
storage events, making development unwieldy when application complexity grows.

On the other hand, driver functions concentrate control flow in a single
function and support arbitrary composition. However, most serverless platforms
impose modest runtime limits on individual functions, and thus driver
functions restrict the total runtime of applications. Furthermore, driver
functions suffer ``double billing'' since they are billed for the entire
call-graph execution despite spending most time idly waiting for callees to
return.

Finally, both ad-hoc approaches require developers to handle function crashes,
retries and duplicate invocations gracefully~\cite{aws-lambda-retry,
azure-functions-retry, aws-lambda-async-invoke,
azure-functions-exec-guarantee}. Application typically want to ensure
``exactly once'' semantics~\cite{netherite, beldi, boki,
formal-foundation-exec-gtnee, durable-semantics} for an entire call-graph, but
failures and multiple invocations of individual functions can subvert this
goal without careful consideration.

\subsection{\remove{Workflow}\majoredits{Standalone} Orchestrators}\label{sec:bg:orchestrator}

\remove{A common solution to address the needs of complex serverless
applications is to introduce a workflow orchestrator~\mbox{\cite{excamera,
gg-atc, aws-step-functions, google-cloud-composer, google-workflows,
durable-functions}}.  Orchestrators operate similarly to
driver-functions---they drive an application workflow by invoking functions in
the application---but are separate hosted services, usually multi-tenant.
Typically, orchestrators provide a high-level programming interface to
describe applications in terms of their control-flow graph and constituent
functions. They offer a rich set of composition primitives, such as branching,
chaining, fan-out and fan-in, while ensuring exactly-once semantics.}

\majoredits{A common solution to address the needs of complex serverless
applications is to introduce a workflow orchestrator that provides a
high-level programming interface with support for a rich set of patterns
(e.g., branching, chaining, fan-out and fan-in)~\cite{excamera, gg-atc,
aws-step-functions, google-cloud-composer, google-workflows,
durable-functions,temporal}. Many cloud providers offer serverless
orchestrators as a service~\cite{aws-step-functions, google-cloud-composer,
google-workflows, durable-functions}. Alternatively, users can build custom
orchestrators~\cite{temporal, gg-atc, excamera} and deploy in VMs alongside
their functions.}

\majoredits{Similar to driver functions, orchestrators operate as
\emph{logically centralized} controllers. They drive a workflow by invoking
its functions and hosting application states such as function outputs and
outstanding invocations.}

\majoredits{However, an important difference of orchestrators is that they are
standalone services. Unlike driver functions, orchestrators are not limited by
function timeouts and can be arbitrarily
long-running~\cite{aws-step-functions-quotas}. Moreover, as standalone services,
orchestrators can be distributed and employ techniques such as sharding and
replication to provide strong execution guarantees and fault-tolerance. For
example, orchestrators can ensure that workflows appear to execute
exactly-once by choosing one result for each function invocation, even if FaaS
engines only guarantee at-least once execution. Orchestrators can also persist
or replicate states during execution so that in face of orchestrator failures,
applications do not lose executions or retry from beginning.}


While \remove{workflow} orchestrators are able to address the needs of complex
serverless applications, introducing a new \remove{platform}
\majoredits{standalone} service has significant drawbacks. Building performant,
scalable \majoredits{and fault-tolerant} multi-tenant systems is hard and
orchestrators introduce yet-another potential performance and scalability
bottleneck. \majoredits{Indeed, we find that, in practice, production systems
limit end-to-end performance for highly-parallel applications
(\S~\ref{sec:eval}).}

\remove{Moreover, hosting such services can be expensive, as they require
dedicated resources in addition to those allocated to FaaS and storage
systems.  Indeed, we find (Section~\ref{sec:eval}) that, in practice,
production systems limit end-to-end performance and scalability, and cost
developers significantly more than necessary in some cases.}
\majoredits{Moreover, hosting such services can be expensive as they require
dedicated resources. Deploying a custom orchestrator per user risks
under-utilization as it cannot multiplex over many users and users pay even
when the orchestrator is not actively in use, breaking the fine-grained
billing benefit of serverless. Provider-hosted orchestrators are multi-tenant
and can amortize this cost. But they still incur engineering expenses as they
require teams on-call. Indeed, we find that provider-hosted orchestrators cost
developers significantly and dominate the total cost of running applications
(\S~\ref{sec:eval}).}

\majoredits{Lastly, provider-hosted orchestrators preclude users from making
application-specific optimizations. Each provider typically offers just a
single orchestrator service option. While the interface and implementation of
the orchestrator might efficiently support many applications, it cannot meet
all applications' needs, resulting in a compromise familiar from operating
systems~\cite{exokernel,spin}, networks~\cite{active-networks,sdn}, and
storage systems~\cite{comet,splinter}. Indeed, we find that provider-hosted
orchestrators force applications to compromise performance by using
less-efficient but supported patterns (\S~\ref{sec:eval}).}

\remove{Indeed, we find (Section~\ref{sec:eval}) that, in practice, production
systems limit end-to-end performance and scalability, and cost developers
significantly more than necessary in some cases.}

% \amit{I can't figure out a way to include developer flexibility here cleanly.
% I'm in favor of adding it, but also it might be a sort of tacit contribution
% that the reader deduces themselves... It could also come out in discussion
% instead. I think it's a really good point, but perhaps not as central to the
% argument? Not sure...}
% %
% \shadi{should we give a cost comparison example here? something in the lines of
% "For example, the pricing schema of AWS step functions makes running a simple X
% app Y times more expensive that running directly on AWS Lambda, and the
% developers have no control on this pricing."}


% \shadi{" Platform-specific orchestrators force developers to write with
% proprietary APIs and locks them in a particular cloud provider." move and
% merge into goals or later section}



%Recently, workflow orchestrators have emerged to tackle the challenges of
%large serverless applications~\cite{excamera, gg-atc, aws-step-functions,
%google-cloud-composer, google-workflows, durable-functions}. Orchestrators are
%separate hosted services that execute workflow definitions, invoke constituent
%functions and manage workflow states. Architecturally, they are similar to
%driver functions (Figure~\ref{fig:chain-example}). For every workflow
%invocation, an orchestrator instance invokes a constituent function, waits for
%its result and passes it to downstream functions via invocation. All functions
%invocation are initiated by the orchestrator and all states (e.g., function
%results) pass through the orchestrator.

%Different from ad-hoc composition, orchestrators offer (1). higher-level
%programming interfaces that directly express function interactions and hide
%low-level APIs, (2). a rich set of composition primitives, including
%branching, chaining, fan-out and fan-in, (3). exactly-once semantics for
%workflow execution, and (4). long or no runtime limits.

%While fixing the flaws of ad-hoc composition, the orchestrator design creates
%several important drawbacks that neglect or even compromise key benefits of
%the serverless abstraction: (1). End-to-end performance now also depends on
%the orchestrator service that is separate from the FaaS engine. A slow
%orchestrator can become a bottleneck and nullify the fast autoscale advantage
%of serverless. (2). Serverless computing already offers compute and storage
%building blocks that are highly performant and scalable. Adding yet another
%separate service that is a mixture of compute and storage is repeating the
%difficult and expensive task of developing and maintaining a large-scale system,
%which often requires a dedicated engineering team.(3). Platform-specific
%orchestrators force developers to write with proprietary APIs and locks them
%in a particular cloud provider.\dhl{I'm hesitant to include the 3rd point
%because 1. we can't demonstrate that \name{} solves this problem. 2. while
%it's a benefit, it feels more an engineering problem than a research problem.
%3. FaaS functions are already platform-specific. Lambda, Azure functions, etc.
%all have different propritary APIs.} \shadi{I thought the point here was to be a design goal of "interoperability".}
%\dhl{Yes, but the problem is that current \name{} implementation only works on AWS. So we can't show/prove that we achieved interoperability if we list that as a goal. }

%\section{Goals}\label{sec:goals}
%In this paper, we examine whether we can have the best of both worlds:
%\emph{Can we preserve the advantages of workflow orchestrators while building
%entirely on top of the existing serverless computing abstraction (no
%supplemental services)?}
%
%Table~\ref{table:positioning} compares the current solutions and \name{}.
%Specifically, we aim for a system that requires \textbf{no new services}. The
%system should work in a basic, unmodified serverless environment and not require
%adding any new services.  The system should execute workflows solely as
%event-driven serverless functions, and use serverless storage, when necessary,
%to enable stateful control-flow patterns.
%
%In addition, we aim to preserve the best properties of existing centralized workflow orchestrators:
%
%\squishlist
%    \item \textbf{High-level API \& support for common interactions.}
%    Developers should compose functions with high-level primitives similar to
%    existing workflow orchestrators instead of ad-hoc mechanisms and be able
%    to simply express common control-flow patterns such as chaining,
%    branching, fan-out and fan-in.
%
%    \item \textbf{Exactly-once semantics.} The system must guarantee workflow
%    progress and consistency. Specifically, it must retry constituent functions on transient faults;
%    yet it must record exactly one result\footnote{The result recorded
%    for a function may be an error result, such as when a retry or time limit is exceeded.}
%    for each function,
%    even if there are multiple executions due to retries.% or duplicate invocations.
%
%    \item \textbf{Comparable performance and costs.} The system should have
%comparable costs and performance as the state-of-the-art workflow orchestrators.
%Similarly, it should avoid idle-billing. Users should only pay for the resources
%used rather than idle time wait for other functions to return.
%
%    \shadi{should we call it idle billing throughout the paper instead of
%    double billing? It is not exactly double...}\dhl{Yeah, I like "idle
%    billing" better. Used "double billing" because of prior work. But if we no
%    longer need to mention this problem anywhere else in the paper other than
%    the driver functions section, I think we can just define and explain the
%    problem and then be done with it.}
%
%\squishend
%
%\shadi{in the table, should we have double billing? maybe another term would
%be better?}\dhl{Changed it to pay-for-what-you-use billing}
%
%
%
%
%%In this paper, we examine whether we can have the best of both worlds. We
%%argue that it is possible to preserve the advantages of workflow orchestrators
%%while building entirely on top of the existing serverless computing
%%abstraction without adding any supplemental components.
%%
%%Specifically, we aim for a system with the following objectives:
%%
%%\paragraph{\emph{Purely serverless}} The system should work within the current
%%serverless environment and not require adding any new services.
%%
%%The system should execute workflows solely as event-driven serverless
%%functions, and use serverless storage, when necessary, to enable stateful
%%operations (e.g., fan-in).
%%
%%\paragraph{Higher-level programming interface} Developers should express
%%function compositions with higher-level primitives similar to exising workflow
%%orchestrators (e.g., AWS Step Functions) instead of ad-hoc mechanisms.
%%
%%\paragraph{Supports all common interactions} The system should support all
%%composition patterns commonly used in orchestrators today, including chaining,
%%branching, fan-out and fan-in.
%%
%%\paragraph{Exactly-once semantics} Even if a function executes multiple times,
%%due to retries or duplicate invocations, concurrently or non-concurrently, the
%%final state should appear that the execution happened only once.
%%
%%\paragraph{Avoids double billing} Users should only pay for the resources
%%used. Functions should not synchronously wait for other functions to return.
%%
%%\paragraph{Comparable performance and costs} The system should at least have
%%comparable costs and performance as the state-of-the-art workflow
%%orchestrators.
